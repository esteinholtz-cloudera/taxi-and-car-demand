{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NYC Taxi Demand vs Used Car Price Analysis (Hybrid: Laptop/Cloud)\n",
    "## Hypothesis: Taxi demand correlates with used car prices in New York areas\n",
    "\n",
    "This notebook analyzes the correlation between NYC taxi trip density (2019-2020) and Craigslist used car prices in NY regions.\n",
    "\n",
    "**Deployment Mode**: Toggle `LAPTOP_DEPLOYMENT` flag below to switch between:\n",
    "- `True`: Pandas-only (Mac laptop friendly, < 16GB RAM recommended)\n",
    "- `False`: Spark + Pandas (Cloudera AI / cluster deployment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”§ DEPLOYMENT CONFIGURATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# TOGGLE THIS FLAG TO SWITCH DEPLOYMENT MODES\n",
    "# ============================================================================\n",
    "LAPTOP_DEPLOYMENT = True  # Set to False for Spark/Cloud deployment\n",
    "# ============================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"  DEPLOYMENT MODE: {'LAPTOP (Pandas)' if LAPTOP_DEPLOYMENT else 'CLOUD (Spark)'}\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "if LAPTOP_DEPLOYMENT:\n",
    "    print(\"âœ“ Running in Laptop mode (Pandas-only)\")\n",
    "    print(\"  - No Spark required\")\n",
    "    print(\"  - Lighter memory footprint\")\n",
    "    print(\"  - Good for datasets < 10GB\")\n",
    "else:\n",
    "    print(\"âœ“ Running in Cloud mode (Spark + Pandas)\")\n",
    "    print(\"  - Distributed processing enabled\")\n",
    "    print(\"  - Scalable for large datasets\")\n",
    "    print(\"  - Optimized for Cloudera AI\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Package Installation\n",
    "\n",
    "**ðŸ’¡ Recommended Alternative:** Use `uv sync` before opening the notebook for faster, reproducible installs!\n",
    "\n",
    "```bash\n",
    "# Laptop mode: uv sync --extra laptop --extra dev\n",
    "# Cloud mode:  uv sync --extra cloud --extra dev\n",
    "# See UV_SETUP.md for details\n",
    "```\n",
    "\n",
    "If you've already run `uv sync`, you can **skip this cell** entirely!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# INSTALLATION METHOD: Choose pip or uv\n",
    "# ============================================================================\n",
    "USE_UV = False  # Set to True to use uv (faster), False for pip\n",
    "# ============================================================================\n",
    "\n",
    "import sys\n",
    "\n",
    "if USE_UV:\n",
    "    # Using uv (faster package installer)\n",
    "    # Install uv first if not already installed: curl -LsSf https://astral.sh/uv/install.sh | sh\n",
    "    print(\"ðŸ“¦ Installing packages with uv (fast mode)...\")\n",
    "    \n",
    "    if LAPTOP_DEPLOYMENT:\n",
    "        # Laptop mode packages\n",
    "        !uv pip install --system kaggle pandas numpy matplotlib seaborn plotly pyarrow fastparquet\n",
    "    else:\n",
    "        # Cloud mode packages\n",
    "        !uv pip install --system kaggle opendatasets pandas numpy matplotlib seaborn plotly\n",
    "else:\n",
    "    # Using standard pip\n",
    "    print(\"ðŸ“¦ Installing packages with pip...\")\n",
    "    \n",
    "    if LAPTOP_DEPLOYMENT:\n",
    "        # Laptop mode packages\n",
    "        !pip install kaggle pandas numpy matplotlib seaborn plotly pyarrow fastparquet -q\n",
    "    else:\n",
    "        # Cloud mode packages\n",
    "        !pip install kaggle opendatasets pandas numpy matplotlib seaborn plotly -q\n",
    "\n",
    "print(\"âœ“ Packages installed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Libraries and Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Pandas environment...\n",
      "âœ“ Pandas version: 2.3.3\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from datetime import datetime\n",
    "\n",
    "if LAPTOP_DEPLOYMENT:\n",
    "    # Laptop mode: Pandas only\n",
    "    print(\"Initializing Pandas environment...\")\n",
    "    # Optimize pandas for larger datasets\n",
    "    pd.options.mode.chained_assignment = None\n",
    "    print(f\"âœ“ Pandas version: {pd.__version__}\")\n",
    "else:\n",
    "    # Cloud mode: Import and initialize Spark\n",
    "    print(\"Initializing Spark environment...\")\n",
    "    from pyspark.sql import SparkSession\n",
    "    from pyspark.sql.functions import *\n",
    "    from pyspark.sql.types import *\n",
    "    from pyspark.sql.window import Window\n",
    "    \n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"TaxiCarPriceCorrelation\") \\\n",
    "        .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "        .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "        .getOrCreate()\n",
    "    \n",
    "    print(f\"âœ“ Spark version: {spark.version}\")\n",
    "    print(f\"âœ“ Pandas version: {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Download Datasets\n",
    "\n",
    "**Note**: You need Kaggle API credentials:\n",
    "1. Go to Kaggle â†’ Account â†’ Create New API Token\n",
    "2. Place `kaggle.json` in `~/.kaggle/` directory\n",
    "3. Run: `chmod 600 ~/.kaggle/kaggle.json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Craigslist cars dataset...\n",
      "Dataset URL: https://www.kaggle.com/datasets/austinreese/craigslist-carstrucks-data\n",
      "License(s): CC0-1.0\n",
      "Downloading craigslist-carstrucks-data.zip to ./data\n",
      " 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ         | 204M/262M [00:00<00:00, 2.14GB/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 262M/262M [00:00<00:00, 2.13GB/s]\n",
      "\n",
      "Downloading NYC Yellow Taxi dataset...\n",
      "Dataset URL: https://www.kaggle.com/datasets/microize/newyork-yellow-taxi-trip-data-2020-2019\n",
      "License(s): ODbL-1.0\n",
      "Downloading newyork-yellow-taxi-trip-data-2020-2019.zip to ./data\n",
      " 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1.78G/1.81G [00:01<00:00, 2.01GB/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.81G/1.81G [00:01<00:00, 1.89GB/s]\n",
      "\n",
      "âœ“ Download complete!\n",
      "total 21200424\n",
      "-rw-r--r--@ 1 eriksteinholz  staff    12K Feb  2 15:06 taxi+_zone_lookup.csv\n",
      "drwxr-xr-x@ 9 eriksteinholz  staff   288B Feb  2 15:06 \u001b[1m\u001b[36mtaxi_zones\u001b[m\u001b[m\n",
      "-rw-r--r--@ 1 eriksteinholz  staff   1.3G Feb  2 15:03 vehicles.csv\n",
      "-rw-r--r--@ 1 eriksteinholz  staff   655M Feb  2 15:06 yellow_tripdata_2019-01.csv\n",
      "-rw-r--r--@ 1 eriksteinholz  staff   620M Feb  2 15:06 yellow_tripdata_2019-02.csv\n",
      "-rw-r--r--@ 1 eriksteinholz  staff   693M Feb  2 15:06 yellow_tripdata_2019-03.csv\n",
      "-rw-r--r--@ 1 eriksteinholz  staff   657M Feb  2 15:06 yellow_tripdata_2019-04.csv\n",
      "-rw-r--r--@ 1 eriksteinholz  staff   669M Feb  2 15:06 yellow_tripdata_2019-05.csv\n",
      "-rw-r--r--@ 1 eriksteinholz  staff   614M Feb  2 15:06 yellow_tripdata_2019-06.csv\n",
      "-rw-r--r--@ 1 eriksteinholz  staff   557M Feb  2 15:06 yellow_tripdata_2019-07.csv\n",
      "-rw-r--r--@ 1 eriksteinholz  staff   536M Feb  2 15:06 yellow_tripdata_2019-08.csv\n",
      "-rw-r--r--@ 1 eriksteinholz  staff   581M Feb  2 15:06 yellow_tripdata_2019-09.csv\n",
      "-rw-r--r--@ 1 eriksteinholz  staff   638M Feb  2 15:06 yellow_tripdata_2019-10.csv\n",
      "-rw-r--r--@ 1 eriksteinholz  staff   608M Feb  2 15:06 yellow_tripdata_2019-11.csv\n",
      "-rw-r--r--@ 1 eriksteinholz  staff   610M Feb  2 15:06 yellow_tripdata_2019-12.csv\n",
      "-rw-r--r--@ 1 eriksteinholz  staff   566M Feb  2 15:06 yellow_tripdata_2020-01.csv\n",
      "-rw-r--r--@ 1 eriksteinholz  staff   557M Feb  2 15:06 yellow_tripdata_2020-02.csv\n",
      "-rw-r--r--@ 1 eriksteinholz  staff   265M Feb  2 15:06 yellow_tripdata_2020-03.csv\n",
      "-rw-r--r--@ 1 eriksteinholz  staff    21M Feb  2 15:06 yellow_tripdata_2020-04.csv\n",
      "-rw-r--r--@ 1 eriksteinholz  staff    30M Feb  2 15:06 yellow_tripdata_2020-05.csv\n",
      "-rw-r--r--@ 1 eriksteinholz  staff    48M Feb  2 15:06 yellow_tripdata_2020-06.csv\n"
     ]
    }
   ],
   "source": [
    "# Create data directory\n",
    "os.makedirs('data', exist_ok=True)\n",
    "\n",
    "# Download Craigslist cars dataset\n",
    "print(\"Downloading Craigslist cars dataset...\")\n",
    "!kaggle datasets download -d austinreese/craigslist-carstrucks-data -p ./data --unzip\n",
    "\n",
    "# Download NYC taxi dataset\n",
    "print(\"\\nDownloading NYC Yellow Taxi dataset...\")\n",
    "!kaggle datasets download -d microize/newyork-yellow-taxi-trip-data-2020-2019 -p ./data --unzip\n",
    "\n",
    "print(\"\\nâœ“ Download complete!\")\n",
    "!ls -lh data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Craigslist dataset...\n",
      "âœ“ Loaded 426,880 records\n",
      "\n",
      "Columns: ['id', 'url', 'region', 'region_url', 'price', 'year', 'manufacturer', 'model', 'condition', 'cylinders', 'fuel', 'odometer', 'title_status', 'transmission', 'VIN', 'drive', 'size', 'type', 'paint_color', 'image_url', 'description', 'county', 'state', 'lat', 'long', 'posting_date']\n",
      "\n",
      "Sample data:\n",
      "           id                                                url  \\\n",
      "0  7222695916  https://prescott.craigslist.org/cto/d/prescott...   \n",
      "1  7218891961  https://fayar.craigslist.org/ctd/d/bentonville...   \n",
      "2  7221797935  https://keys.craigslist.org/cto/d/summerland-k...   \n",
      "3  7222270760  https://worcester.craigslist.org/cto/d/west-br...   \n",
      "4  7210384030  https://greensboro.craigslist.org/cto/d/trinit...   \n",
      "\n",
      "                   region                         region_url  price  year  \\\n",
      "0                prescott    https://prescott.craigslist.org   6000   NaN   \n",
      "1            fayetteville       https://fayar.craigslist.org  11900   NaN   \n",
      "2            florida keys        https://keys.craigslist.org  21000   NaN   \n",
      "3  worcester / central MA   https://worcester.craigslist.org   1500   NaN   \n",
      "4              greensboro  https://greensboro.craigslist.org   4900   NaN   \n",
      "\n",
      "  manufacturer model condition cylinders  ... size  type paint_color  \\\n",
      "0          NaN   NaN       NaN       NaN  ...  NaN   NaN         NaN   \n",
      "1          NaN   NaN       NaN       NaN  ...  NaN   NaN         NaN   \n",
      "2          NaN   NaN       NaN       NaN  ...  NaN   NaN         NaN   \n",
      "3          NaN   NaN       NaN       NaN  ...  NaN   NaN         NaN   \n",
      "4          NaN   NaN       NaN       NaN  ...  NaN   NaN         NaN   \n",
      "\n",
      "  image_url description county state lat long posting_date  \n",
      "0       NaN         NaN    NaN    az NaN  NaN          NaN  \n",
      "1       NaN         NaN    NaN    ar NaN  NaN          NaN  \n",
      "2       NaN         NaN    NaN    fl NaN  NaN          NaN  \n",
      "3       NaN         NaN    NaN    ma NaN  NaN          NaN  \n",
      "4       NaN         NaN    NaN    nc NaN  NaN          NaN  \n",
      "\n",
      "[5 rows x 26 columns]\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading Craigslist dataset...\")\n",
    "\n",
    "if LAPTOP_DEPLOYMENT:\n",
    "    # Laptop: Use Pandas with optimized dtypes\n",
    "    craigslist_df = pd.read_csv(\n",
    "        \"data/vehicles.csv\",\n",
    "        low_memory=False\n",
    "    )\n",
    "    print(f\"âœ“ Loaded {len(craigslist_df):,} records\")\n",
    "    print(f\"\\nColumns: {list(craigslist_df.columns)}\")\n",
    "    print(f\"\\nSample data:\")\n",
    "    print(craigslist_df.head())\n",
    "else:\n",
    "    # Cloud: Use Spark\n",
    "    craigslist_df = spark.read.csv(\n",
    "        \"data/vehicles.csv\",\n",
    "        header=True,\n",
    "        inferSchema=True\n",
    "    )\n",
    "    print(\"Craigslist Dataset Schema:\")\n",
    "    craigslist_df.printSchema()\n",
    "    print(f\"\\nâœ“ Total records: {craigslist_df.count():,}\")\n",
    "    print(\"\\nSample data:\")\n",
    "    craigslist_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "States in Craigslist dataset:\n",
      "state\n",
      "ca    50614\n",
      "fl    28511\n",
      "tx    22945\n",
      "ny    19386\n",
      "oh    17696\n",
      "or    17104\n",
      "mi    16900\n",
      "nc    15277\n",
      "wa    13861\n",
      "pa    13753\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check available states\n",
    "print(\"States in Craigslist dataset:\")\n",
    "\n",
    "if LAPTOP_DEPLOYMENT:\n",
    "    state_counts = craigslist_df['state'].value_counts().head(10)\n",
    "    print(state_counts)\n",
    "else:\n",
    "    craigslist_df.groupBy(\"state\").count().orderBy(desc(\"count\")).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading NYC Taxi dataset...\n"
     ]
    },
    {
     "ename": "ArrowKeyError",
     "evalue": "No type extension with name arrow.py_extension_type found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mArrowKeyError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m         taxi_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([pd\u001b[38;5;241m.\u001b[39mread_parquet(f) \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m parquet_files], ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 15\u001b[0m         taxi_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     taxi_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/yellow_tripdata_2019-2020.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, low_memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/src/jupyter-kaggle-question-prompting/.venv/lib/python3.10/site-packages/pandas/io/parquet.py:653\u001b[0m, in \u001b[0;36mread_parquet\u001b[0;34m(path, engine, columns, storage_options, use_nullable_dtypes, dtype_backend, filesystem, filters, **kwargs)\u001b[0m\n\u001b[1;32m    500\u001b[0m \u001b[38;5;129m@doc\u001b[39m(storage_options\u001b[38;5;241m=\u001b[39m_shared_docs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    501\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mread_parquet\u001b[39m(\n\u001b[1;32m    502\u001b[0m     path: FilePath \u001b[38;5;241m|\u001b[39m ReadBuffer[\u001b[38;5;28mbytes\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    511\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame:\n\u001b[1;32m    512\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    513\u001b[0m \u001b[38;5;124;03m    Load a parquet object from the file path, returning a DataFrame.\u001b[39;00m\n\u001b[1;32m    514\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    650\u001b[0m \u001b[38;5;124;03m    1    4    9\u001b[39;00m\n\u001b[1;32m    651\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 653\u001b[0m     impl \u001b[38;5;241m=\u001b[39m \u001b[43mget_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    655\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m use_nullable_dtypes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mno_default:\n\u001b[1;32m    656\u001b[0m         msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    657\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe argument \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muse_nullable_dtypes\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is deprecated and will be removed \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    658\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min a future version.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    659\u001b[0m         )\n",
      "File \u001b[0;32m~/src/jupyter-kaggle-question-prompting/.venv/lib/python3.10/site-packages/pandas/io/parquet.py:64\u001b[0m, in \u001b[0;36mget_engine\u001b[0;34m(engine)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m engine_class \u001b[38;5;129;01min\u001b[39;00m engine_classes:\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 64\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mengine_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m     66\u001b[0m         error_msgs \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m - \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(err)\n",
      "File \u001b[0;32m~/src/jupyter-kaggle-question-prompting/.venv/lib/python3.10/site-packages/pandas/io/parquet.py:170\u001b[0m, in \u001b[0;36mPyArrowImpl.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpyarrow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparquet\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;66;03m# import utils to register the pyarrow extension types\u001b[39;00m\n\u001b[0;32m--> 170\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrays\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mextension_types\u001b[39;00m  \u001b[38;5;66;03m# pyright: ignore[reportUnusedImport] # noqa: F401\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi \u001b[38;5;241m=\u001b[39m pyarrow\n",
      "File \u001b[0;32m~/src/jupyter-kaggle-question-prompting/.venv/lib/python3.10/site-packages/pandas/core/arrays/arrow/extension_types.py:174\u001b[0m\n\u001b[1;32m    167\u001b[0m     pyarrow\u001b[38;5;241m.\u001b[39mregister_extension_type(\n\u001b[1;32m    168\u001b[0m         ForbiddenExtensionType(pyarrow\u001b[38;5;241m.\u001b[39mnull(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marrow.py_extension_type\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    169\u001b[0m     )\n\u001b[1;32m    171\u001b[0m     pyarrow\u001b[38;5;241m.\u001b[39m_hotfix_installed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 174\u001b[0m \u001b[43mpatch_pyarrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/src/jupyter-kaggle-question-prompting/.venv/lib/python3.10/site-packages/pandas/core/arrays/arrow/extension_types.py:166\u001b[0m, in \u001b[0;36mpatch_pyarrow\u001b[0;34m()\u001b[0m\n\u001b[1;32m    157\u001b[0m         pickletools\u001b[38;5;241m.\u001b[39mdis(serialized, out)\n\u001b[1;32m    158\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    159\u001b[0m             _ERROR_MSG\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    160\u001b[0m                 storage_type\u001b[38;5;241m=\u001b[39mstorage_type,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    163\u001b[0m             )\n\u001b[1;32m    164\u001b[0m         )\n\u001b[0;32m--> 166\u001b[0m \u001b[43mpyarrow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munregister_extension_type\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43marrow.py_extension_type\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m pyarrow\u001b[38;5;241m.\u001b[39mregister_extension_type(\n\u001b[1;32m    168\u001b[0m     ForbiddenExtensionType(pyarrow\u001b[38;5;241m.\u001b[39mnull(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marrow.py_extension_type\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    169\u001b[0m )\n\u001b[1;32m    171\u001b[0m pyarrow\u001b[38;5;241m.\u001b[39m_hotfix_installed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/src/jupyter-kaggle-question-prompting/.venv/lib/python3.10/site-packages/pyarrow/types.pxi:2280\u001b[0m, in \u001b[0;36mpyarrow.lib.unregister_extension_type\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/src/jupyter-kaggle-question-prompting/.venv/lib/python3.10/site-packages/pyarrow/error.pxi:92\u001b[0m, in \u001b[0;36mpyarrow.lib.check_status\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mArrowKeyError\u001b[0m: No type extension with name arrow.py_extension_type found"
     ]
    }
   ],
   "source": [
    "print(\"\\nLoading NYC Taxi dataset...\")\n",
    "\n",
    "# Detect file format\n",
    "taxi_files = !ls data/*.parquet data/*.csv 2>/dev/null || echo \"no_files\"\n",
    "is_parquet = any('.parquet' in f for f in taxi_files)\n",
    "\n",
    "if LAPTOP_DEPLOYMENT:\n",
    "    # Laptop: Use Pandas\n",
    "    if is_parquet:\n",
    "        import glob\n",
    "        parquet_files = glob.glob(\"data/yellow_*.parquet\") or glob.glob(\"data/*.parquet\")\n",
    "        if parquet_files:\n",
    "            taxi_df = pd.concat([pd.read_parquet(f) for f in parquet_files], ignore_index=True)\n",
    "        else:\n",
    "            taxi_df = pd.read_parquet(\"data/\")\n",
    "    else:\n",
    "        taxi_df = pd.read_csv(\"data/yellow_tripdata_2019-2020.csv\", low_memory=False)\n",
    "    \n",
    "    print(f\"âœ“ Loaded {len(taxi_df):,} records\")\n",
    "    print(f\"\\nColumns: {list(taxi_df.columns)}\")\n",
    "    print(f\"\\nSample data:\")\n",
    "    print(taxi_df.head())\n",
    "else:\n",
    "    # Cloud: Use Spark\n",
    "    taxi_path = \"data/yellow_*.parquet\" if is_parquet else \"data/*.csv\"\n",
    "    \n",
    "    if is_parquet:\n",
    "        taxi_df = spark.read.parquet(taxi_path)\n",
    "    else:\n",
    "        taxi_df = spark.read.csv(taxi_path, header=True, inferSchema=True)\n",
    "    \n",
    "    print(\"NYC Taxi Dataset Schema:\")\n",
    "    taxi_df.printSchema()\n",
    "    print(f\"\\nâœ“ Total records: {taxi_df.count():,}\")\n",
    "    print(\"\\nSample data:\")\n",
    "    taxi_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Preparation: Craigslist Cars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Filter to NY State and 2019-2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Filtering Craigslist data to NY state, 2019-2020...\")\n",
    "\n",
    "if LAPTOP_DEPLOYMENT:\n",
    "    # Laptop: Pandas operations\n",
    "    ny_cars_raw = craigslist_df[craigslist_df['state'] == 'ny'].copy()\n",
    "    print(f\"âœ“ NY car listings: {len(ny_cars_raw):,}\")\n",
    "    \n",
    "    # Convert date column\n",
    "    ny_cars_raw['posting_date'] = pd.to_datetime(ny_cars_raw['posting_date'], errors='coerce')\n",
    "    \n",
    "    # Show date range\n",
    "    print(f\"\\nDate range: {ny_cars_raw['posting_date'].min()} to {ny_cars_raw['posting_date'].max()}\")\n",
    "    \n",
    "else:\n",
    "    # Cloud: Spark operations\n",
    "    ny_cars_raw = craigslist_df.filter(col(\"state\") == \"ny\")\n",
    "    print(f\"âœ“ NY car listings: {ny_cars_raw.count():,}\")\n",
    "    \n",
    "    print(\"\\nDate range in dataset:\")\n",
    "    ny_cars_raw.select(\n",
    "        min(\"posting_date\").alias(\"min_date\"),\n",
    "        max(\"posting_date\").alias(\"max_date\")\n",
    "    ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Cleaning and filtering to 2019-2020...\")\n",
    "\n",
    "if LAPTOP_DEPLOYMENT:\n",
    "    # Laptop: Pandas operations\n",
    "    ny_cars_cleaned = ny_cars_raw[\n",
    "        ny_cars_raw['price'].notna() &\n",
    "        ny_cars_raw['year'].notna() &\n",
    "        ny_cars_raw['posting_date'].notna()\n",
    "    ].copy()\n",
    "    \n",
    "    # Filter to 2019-2020\n",
    "    ny_cars_cleaned = ny_cars_cleaned[\n",
    "        ny_cars_cleaned['posting_date'].dt.year.isin([2019, 2020])\n",
    "    ]\n",
    "    \n",
    "    # Convert types\n",
    "    ny_cars_cleaned['price'] = pd.to_numeric(ny_cars_cleaned['price'], errors='coerce')\n",
    "    ny_cars_cleaned['year'] = pd.to_numeric(ny_cars_cleaned['year'], errors='coerce').astype('Int64')\n",
    "    ny_cars_cleaned['odometer'] = pd.to_numeric(ny_cars_cleaned['odometer'], errors='coerce')\n",
    "    \n",
    "    print(f\"âœ“ After filtering 2019-2020: {len(ny_cars_cleaned):,} records\")\n",
    "    \n",
    "else:\n",
    "    # Cloud: Spark operations\n",
    "    ny_cars_cleaned = ny_cars_raw \\\n",
    "        .filter(col(\"price\").isNotNull()) \\\n",
    "        .filter(col(\"year\").isNotNull()) \\\n",
    "        .filter(col(\"posting_date\").isNotNull()) \\\n",
    "        .withColumn(\"posting_date\", to_timestamp(col(\"posting_date\"))) \\\n",
    "        .filter(year(col(\"posting_date\")).isin([2019, 2020])) \\\n",
    "        .withColumn(\"price\", col(\"price\").cast(\"double\")) \\\n",
    "        .withColumn(\"year\", col(\"year\").cast(\"int\")) \\\n",
    "        .withColumn(\"odometer\", col(\"odometer\").cast(\"double\"))\n",
    "    \n",
    "    print(f\"âœ“ After filtering 2019-2020: {ny_cars_cleaned.count():,} records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Calculate Statistics and Remove Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Calculating price statistics...\")\n",
    "\n",
    "if LAPTOP_DEPLOYMENT:\n",
    "    # Laptop: Pandas statistics\n",
    "    price_stats = {\n",
    "        'mean_price': ny_cars_cleaned['price'].mean(),\n",
    "        'std_price': ny_cars_cleaned['price'].std(),\n",
    "        'min_price': ny_cars_cleaned['price'].min(),\n",
    "        'max_price': ny_cars_cleaned['price'].max(),\n",
    "        'p01': ny_cars_cleaned['price'].quantile(0.01),\n",
    "        'p99': ny_cars_cleaned['price'].quantile(0.99)\n",
    "    }\n",
    "else:\n",
    "    # Cloud: Spark statistics\n",
    "    price_stats = ny_cars_cleaned.select(\n",
    "        mean(\"price\").alias(\"mean_price\"),\n",
    "        stddev(\"price\").alias(\"std_price\"),\n",
    "        expr(\"percentile_approx(price, 0.01)\").alias(\"p01\"),\n",
    "        expr(\"percentile_approx(price, 0.99)\").alias(\"p99\"),\n",
    "        min(\"price\").alias(\"min_price\"),\n",
    "        max(\"price\").alias(\"max_price\")\n",
    "    ).collect()[0].asDict()\n",
    "\n",
    "print(\"\\n=== PRICE STATISTICS ===\")\n",
    "print(f\"Mean: ${price_stats['mean_price']:,.2f}\")\n",
    "print(f\"Std Dev: ${price_stats['std_price']:,.2f}\")\n",
    "print(f\"Min: ${price_stats['min_price']:,.2f}\")\n",
    "print(f\"Max: ${price_stats['max_price']:,.2f}\")\n",
    "print(f\"1st percentile: ${price_stats['p01']:,.2f}\")\n",
    "print(f\"99th percentile: ${price_stats['p99']:,.2f}\")\n",
    "\n",
    "# Define outlier thresholds\n",
    "price_lower = max(500, price_stats['p01'])\n",
    "price_upper = min(75000, price_stats['p99'])\n",
    "\n",
    "print(f\"\\nâœ“ Outlier thresholds: ${price_lower:,.2f} - ${price_upper:,.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Removing outliers...\")\n",
    "\n",
    "if LAPTOP_DEPLOYMENT:\n",
    "    # Laptop: Pandas filtering\n",
    "    outliers_mask = (ny_cars_cleaned['price'] < price_lower) | (ny_cars_cleaned['price'] > price_upper)\n",
    "    outliers_removed = ny_cars_cleaned[outliers_mask]\n",
    "    ny_cars = ny_cars_cleaned[~outliers_mask].copy()\n",
    "    \n",
    "    print(f\"\\n=== OUTLIER REMOVAL REPORT ===\")\n",
    "    print(f\"Records before: {len(ny_cars_cleaned):,}\")\n",
    "    print(f\"Outliers removed: {len(outliers_removed):,} ({len(outliers_removed)/len(ny_cars_cleaned)*100:.2f}%)\")\n",
    "    print(f\"Records after: {len(ny_cars):,}\")\n",
    "    \n",
    "    print(\"\\nOutlier breakdown:\")\n",
    "    too_low = (outliers_removed['price'] < price_lower).sum()\n",
    "    too_high = (outliers_removed['price'] > price_upper).sum()\n",
    "    print(f\"  Too low (< ${price_lower:,.0f}): {too_low:,}\")\n",
    "    print(f\"  Too high (> ${price_upper:,.0f}): {too_high:,}\")\n",
    "    \n",
    "else:\n",
    "    # Cloud: Spark filtering\n",
    "    outliers_removed = ny_cars_cleaned.filter(\n",
    "        (col(\"price\") < price_lower) | (col(\"price\") > price_upper)\n",
    "    )\n",
    "    ny_cars = ny_cars_cleaned.filter(\n",
    "        (col(\"price\") >= price_lower) & (col(\"price\") <= price_upper)\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n=== OUTLIER REMOVAL REPORT ===\")\n",
    "    print(f\"Records before: {ny_cars_cleaned.count():,}\")\n",
    "    print(f\"Outliers removed: {outliers_removed.count():,} ({outliers_removed.count()/ny_cars_cleaned.count()*100:.2f}%)\")\n",
    "    print(f\"Records after: {ny_cars.count():,}\")\n",
    "    \n",
    "    print(\"\\nOutlier breakdown:\")\n",
    "    outliers_removed.select(\n",
    "        count(when(col(\"price\") < price_lower, 1)).alias(\"too_low\"),\n",
    "        count(when(col(\"price\") > price_upper, 1)).alias(\"too_high\")\n",
    "    ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Add Derived Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Adding derived columns for dimensional analysis...\")\n",
    "\n",
    "if LAPTOP_DEPLOYMENT:\n",
    "    # Laptop: Pandas operations\n",
    "    ny_cars_final = ny_cars.copy()\n",
    "    \n",
    "    # Date dimensions\n",
    "    ny_cars_final['date'] = ny_cars_final['posting_date'].dt.date\n",
    "    ny_cars_final['year_month'] = ny_cars_final['posting_date'].dt.strftime('%Y-%m')\n",
    "    ny_cars_final['year_week'] = ny_cars_final['posting_date'].dt.strftime('%Y-%U')\n",
    "    \n",
    "    # Vehicle age\n",
    "    ny_cars_final['vehicle_age'] = ny_cars_final['posting_date'].dt.year - ny_cars_final['year']\n",
    "    ny_cars_final['age_category'] = pd.cut(\n",
    "        ny_cars_final['vehicle_age'],\n",
    "        bins=[-np.inf, 3, 7, 12, np.inf],\n",
    "        labels=['0-3 years', '4-7 years', '8-12 years', '12+ years']\n",
    "    )\n",
    "    \n",
    "    # Clean text fields\n",
    "    ny_cars_final['vehicle_type_clean'] = ny_cars_final['type'].fillna('unknown').str.lower().str.strip()\n",
    "    ny_cars_final['manufacturer_clean'] = ny_cars_final['manufacturer'].fillna('unknown').str.lower().str.strip()\n",
    "    ny_cars_final['region_clean'] = ny_cars_final['region'].fillna('unknown').str.lower().str.strip()\n",
    "    \n",
    "    print(f\"âœ“ Final car dataset: {len(ny_cars_final):,} records\")\n",
    "    print(\"\\nSample:\")\n",
    "    print(ny_cars_final[[\n",
    "        'date', 'year_month', 'price', 'manufacturer_clean',\n",
    "        'vehicle_type_clean', 'age_category', 'region_clean'\n",
    "    ]].head(10))\n",
    "    \n",
    "else:\n",
    "    # Cloud: Spark operations\n",
    "    ny_cars_final = ny_cars \\\n",
    "        .withColumn(\"date\", to_date(col(\"posting_date\"))) \\\n",
    "        .withColumn(\"year_month\", date_format(col(\"posting_date\"), \"yyyy-MM\")) \\\n",
    "        .withColumn(\"year_week\", date_format(col(\"posting_date\"), \"yyyy-ww\")) \\\n",
    "        .withColumn(\"vehicle_age\", year(col(\"posting_date\")) - col(\"year\")) \\\n",
    "        .withColumn(\n",
    "            \"age_category\",\n",
    "            when(col(\"vehicle_age\") <= 3, \"0-3 years\")\n",
    "            .when(col(\"vehicle_age\") <= 7, \"4-7 years\")\n",
    "            .when(col(\"vehicle_age\") <= 12, \"8-12 years\")\n",
    "            .otherwise(\"12+ years\")\n",
    "        ) \\\n",
    "        .withColumn(\n",
    "            \"vehicle_type_clean\",\n",
    "            coalesce(lower(trim(col(\"type\"))), lit(\"unknown\"))\n",
    "        ) \\\n",
    "        .withColumn(\n",
    "            \"manufacturer_clean\",\n",
    "            coalesce(lower(trim(col(\"manufacturer\"))), lit(\"unknown\"))\n",
    "        ) \\\n",
    "        .withColumn(\n",
    "            \"region_clean\",\n",
    "            coalesce(lower(trim(col(\"region\"))), lit(\"unknown\"))\n",
    "        )\n",
    "    \n",
    "    print(\"âœ“ Final car dataset prepared\")\n",
    "    ny_cars_final.select(\n",
    "        \"date\", \"year_month\", \"price\", \"manufacturer_clean\",\n",
    "        \"vehicle_type_clean\", \"age_category\", \"region_clean\"\n",
    "    ).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Data Preparation: NYC Taxi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Identify Columns and Filter 2019-2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Identifying datetime columns...\")\n",
    "\n",
    "if LAPTOP_DEPLOYMENT:\n",
    "    taxi_columns = list(taxi_df.columns)\n",
    "else:\n",
    "    taxi_columns = taxi_df.columns\n",
    "\n",
    "pickup_col = [c for c in taxi_columns if 'pickup' in c.lower() and 'datetime' in c.lower()][0]\n",
    "dropoff_col = [c for c in taxi_columns if 'dropoff' in c.lower() and 'datetime' in c.lower()][0]\n",
    "\n",
    "print(f\"âœ“ Using columns: {pickup_col}, {dropoff_col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Cleaning and filtering taxi data to 2019-2020...\")\n",
    "\n",
    "if LAPTOP_DEPLOYMENT:\n",
    "    # Laptop: Pandas operations\n",
    "    taxi_cleaned = taxi_df[taxi_df[pickup_col].notna()].copy()\n",
    "    \n",
    "    # Convert datetime\n",
    "    taxi_cleaned['pickup_datetime'] = pd.to_datetime(taxi_cleaned[pickup_col], errors='coerce')\n",
    "    taxi_cleaned['dropoff_datetime'] = pd.to_datetime(taxi_cleaned[dropoff_col], errors='coerce')\n",
    "    \n",
    "    # Filter to 2019-2020\n",
    "    taxi_cleaned = taxi_cleaned[\n",
    "        taxi_cleaned['pickup_datetime'].dt.year.isin([2019, 2020]) &\n",
    "        (taxi_cleaned['pickup_datetime'] < taxi_cleaned['dropoff_datetime'])\n",
    "    ]\n",
    "    \n",
    "    print(f\"âœ“ Taxi records in 2019-2020: {len(taxi_cleaned):,}\")\n",
    "    \n",
    "else:\n",
    "    # Cloud: Spark operations\n",
    "    taxi_cleaned = taxi_df \\\n",
    "        .filter(col(pickup_col).isNotNull()) \\\n",
    "        .withColumn(\"pickup_datetime\", to_timestamp(col(pickup_col))) \\\n",
    "        .withColumn(\"dropoff_datetime\", to_timestamp(col(dropoff_col))) \\\n",
    "        .filter(year(col(\"pickup_datetime\")).isin([2019, 2020])) \\\n",
    "        .filter(col(\"pickup_datetime\") < col(\"dropoff_datetime\"))\n",
    "    \n",
    "    print(f\"âœ“ Taxi records in 2019-2020: {taxi_cleaned.count():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Remove Distance Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for distance column\n",
    "distance_col = None\n",
    "for col_name in ['trip_distance', 'distance']:\n",
    "    if col_name in taxi_columns:\n",
    "        distance_col = col_name\n",
    "        break\n",
    "\n",
    "if distance_col:\n",
    "    print(f\"Removing trip distance outliers (using column: {distance_col})...\")\n",
    "    \n",
    "    if LAPTOP_DEPLOYMENT:\n",
    "        # Laptop: Pandas operations\n",
    "        p99_distance = taxi_cleaned[distance_col].quantile(0.99)\n",
    "        max_trip_distance = min(50, p99_distance)\n",
    "        \n",
    "        outliers_count = (\n",
    "            (taxi_cleaned[distance_col] <= 0) | \n",
    "            (taxi_cleaned[distance_col] > max_trip_distance)\n",
    "        ).sum()\n",
    "        \n",
    "        taxi_cleaned = taxi_cleaned[\n",
    "            (taxi_cleaned[distance_col] > 0) & \n",
    "            (taxi_cleaned[distance_col] <= max_trip_distance)\n",
    "        ]\n",
    "        \n",
    "        print(f\"\\n=== TAXI OUTLIER REMOVAL ===\")\n",
    "        print(f\"Outliers removed: {outliers_count:,}\")\n",
    "        print(f\"âœ“ Remaining records: {len(taxi_cleaned):,}\")\n",
    "        \n",
    "    else:\n",
    "        # Cloud: Spark operations\n",
    "        taxi_distance_stats = taxi_cleaned.select(\n",
    "            expr(f\"percentile_approx({distance_col}, 0.99)\").alias(\"p99_distance\"),\n",
    "            max(distance_col).alias(\"max_distance\")\n",
    "        ).collect()[0]\n",
    "        \n",
    "        max_trip_distance = min(50, taxi_distance_stats['p99_distance'])\n",
    "        \n",
    "        outliers_taxi = taxi_cleaned.filter(\n",
    "            (col(distance_col) <= 0) | (col(distance_col) > max_trip_distance)\n",
    "        )\n",
    "        \n",
    "        taxi_cleaned = taxi_cleaned.filter(\n",
    "            (col(distance_col) > 0) & (col(distance_col) <= max_trip_distance)\n",
    "        )\n",
    "        \n",
    "        print(f\"\\n=== TAXI OUTLIER REMOVAL ===\")\n",
    "        print(f\"Outliers removed: {outliers_taxi.count():,}\")\n",
    "        print(f\"âœ“ Remaining records: {taxi_cleaned.count():,}\")\n",
    "else:\n",
    "    print(\"âš  No distance column found, skipping distance outlier removal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Add Time Dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Adding time dimensions to taxi data...\")\n",
    "\n",
    "if LAPTOP_DEPLOYMENT:\n",
    "    # Laptop: Pandas operations\n",
    "    taxi_final = taxi_cleaned.copy()\n",
    "    \n",
    "    taxi_final['pickup_date'] = taxi_final['pickup_datetime'].dt.date\n",
    "    taxi_final['pickup_year_month'] = taxi_final['pickup_datetime'].dt.strftime('%Y-%m')\n",
    "    taxi_final['pickup_year_week'] = taxi_final['pickup_datetime'].dt.strftime('%Y-%U')\n",
    "    taxi_final['pickup_hour'] = taxi_final['pickup_datetime'].dt.hour\n",
    "    taxi_final['dropoff_date'] = taxi_final['dropoff_datetime'].dt.date\n",
    "    taxi_final['trip_duration_min'] = (\n",
    "        (taxi_final['dropoff_datetime'] - taxi_final['pickup_datetime']).dt.total_seconds() / 60\n",
    "    )\n",
    "    \n",
    "    print(f\"âœ“ Taxi dataset prepared: {len(taxi_final):,} records\")\n",
    "    print(\"\\nSample:\")\n",
    "    print(taxi_final[['pickup_date', 'pickup_year_month', 'pickup_hour', 'trip_duration_min']].head(10))\n",
    "    \n",
    "else:\n",
    "    # Cloud: Spark operations\n",
    "    taxi_final = taxi_cleaned \\\n",
    "        .withColumn(\"pickup_date\", to_date(col(\"pickup_datetime\"))) \\\n",
    "        .withColumn(\"pickup_year_month\", date_format(col(\"pickup_datetime\"), \"yyyy-MM\")) \\\n",
    "        .withColumn(\"pickup_year_week\", date_format(col(\"pickup_datetime\"), \"yyyy-ww\")) \\\n",
    "        .withColumn(\"pickup_hour\", hour(col(\"pickup_datetime\"))) \\\n",
    "        .withColumn(\"dropoff_date\", to_date(col(\"dropoff_datetime\"))) \\\n",
    "        .withColumn(\"trip_duration_min\",\n",
    "                    (unix_timestamp(col(\"dropoff_datetime\")) - unix_timestamp(col(\"pickup_datetime\"))) / 60)\n",
    "    \n",
    "    print(\"âœ“ Taxi dataset prepared\")\n",
    "    taxi_final.select(\n",
    "        \"pickup_date\", \"pickup_year_month\", \"pickup_hour\", \"trip_duration_min\"\n",
    "    ).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Build Dimensional Model - Taxi Demand Facts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Creating taxi demand aggregations...\")\n",
    "\n",
    "if LAPTOP_DEPLOYMENT:\n",
    "    # Laptop: Pandas aggregations\n",
    "    \n",
    "    # Daily pickup density\n",
    "    pickup_density_daily = taxi_final.groupby('pickup_date').agg({\n",
    "        'pickup_datetime': 'count',\n",
    "        'trip_duration_min': 'mean'\n",
    "    }).rename(columns={\n",
    "        'pickup_datetime': 'pickup_trip_count',\n",
    "        'trip_duration_min': 'avg_trip_duration'\n",
    "    }).reset_index().rename(columns={'pickup_date': 'date'})\n",
    "    \n",
    "    # Daily dropoff density\n",
    "    dropoff_density_daily = taxi_final.groupby('dropoff_date').agg({\n",
    "        'dropoff_datetime': 'count'\n",
    "    }).rename(columns={\n",
    "        'dropoff_datetime': 'dropoff_trip_count'\n",
    "    }).reset_index().rename(columns={'dropoff_date': 'date'})\n",
    "    \n",
    "    # Merge\n",
    "    taxi_demand_daily = pickup_density_daily.merge(\n",
    "        dropoff_density_daily, on='date', how='outer'\n",
    "    ).fillna(0)\n",
    "    \n",
    "    taxi_demand_daily['total_trip_count'] = (\n",
    "        taxi_demand_daily['pickup_trip_count'] + taxi_demand_daily['dropoff_trip_count']\n",
    "    )\n",
    "    taxi_demand_daily['date'] = pd.to_datetime(taxi_demand_daily['date'])\n",
    "    taxi_demand_daily['year_month'] = taxi_demand_daily['date'].dt.strftime('%Y-%m')\n",
    "    taxi_demand_daily['year_week'] = taxi_demand_daily['date'].dt.strftime('%Y-%U')\n",
    "    \n",
    "    print(f\"âœ“ Daily taxi demand records: {len(taxi_demand_daily):,}\")\n",
    "    print(\"\\nSample:\")\n",
    "    print(taxi_demand_daily.head(10))\n",
    "    \n",
    "else:\n",
    "    # Cloud: Spark aggregations\n",
    "    pickup_density_daily = taxi_final \\\n",
    "        .groupBy(\"pickup_date\") \\\n",
    "        .agg(\n",
    "            count(\"*\").alias(\"pickup_trip_count\"),\n",
    "            avg(\"trip_duration_min\").alias(\"avg_trip_duration\")\n",
    "        ) \\\n",
    "        .withColumn(\"trip_type\", lit(\"pickup\"))\n",
    "    \n",
    "    dropoff_density_daily = taxi_final \\\n",
    "        .groupBy(\"dropoff_date\") \\\n",
    "        .agg(\n",
    "            count(\"*\").alias(\"dropoff_trip_count\"),\n",
    "        ) \\\n",
    "        .withColumnRenamed(\"dropoff_date\", \"date\")\n",
    "    \n",
    "    taxi_demand_daily = pickup_density_daily \\\n",
    "        .withColumnRenamed(\"pickup_date\", \"date\") \\\n",
    "        .join(dropoff_density_daily, \"date\", \"outer\") \\\n",
    "        .fillna(0) \\\n",
    "        .withColumn(\"total_trip_count\", col(\"pickup_trip_count\") + col(\"dropoff_trip_count\")) \\\n",
    "        .withColumn(\"year_month\", date_format(col(\"date\"), \"yyyy-MM\")) \\\n",
    "        .withColumn(\"year_week\", date_format(col(\"date\"), \"yyyy-ww\"))\n",
    "    \n",
    "    print(f\"âœ“ Daily taxi demand records: {taxi_demand_daily.count():,}\")\n",
    "    taxi_demand_daily.orderBy(\"date\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weekly aggregation\n",
    "print(\"Creating weekly aggregation...\")\n",
    "\n",
    "if LAPTOP_DEPLOYMENT:\n",
    "    taxi_demand_weekly = taxi_demand_daily.groupby('year_week').agg({\n",
    "        'pickup_trip_count': 'sum',\n",
    "        'dropoff_trip_count': 'sum',\n",
    "        'total_trip_count': 'sum',\n",
    "        'avg_trip_duration': 'mean'\n",
    "    }).reset_index()\n",
    "    \n",
    "    print(f\"âœ“ Weekly taxi demand records: {len(taxi_demand_weekly):,}\")\n",
    "else:\n",
    "    taxi_demand_weekly = taxi_demand_daily \\\n",
    "        .groupBy(\"year_week\") \\\n",
    "        .agg(\n",
    "            sum(\"pickup_trip_count\").alias(\"pickup_trip_count\"),\n",
    "            sum(\"dropoff_trip_count\").alias(\"dropoff_trip_count\"),\n",
    "            sum(\"total_trip_count\").alias(\"total_trip_count\"),\n",
    "            avg(\"avg_trip_duration\").alias(\"avg_trip_duration\")\n",
    "        )\n",
    "    \n",
    "    print(f\"âœ“ Weekly taxi demand records: {taxi_demand_weekly.count():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monthly aggregation\n",
    "print(\"Creating monthly aggregation...\")\n",
    "\n",
    "if LAPTOP_DEPLOYMENT:\n",
    "    taxi_demand_monthly = taxi_demand_daily.groupby('year_month').agg({\n",
    "        'pickup_trip_count': 'sum',\n",
    "        'dropoff_trip_count': 'sum',\n",
    "        'total_trip_count': 'sum',\n",
    "        'avg_trip_duration': 'mean',\n",
    "        'date': 'count'\n",
    "    }).rename(columns={'date': 'days_in_period'}).reset_index()\n",
    "    \n",
    "    taxi_demand_monthly['avg_daily_trips'] = (\n",
    "        taxi_demand_monthly['total_trip_count'] / taxi_demand_monthly['days_in_period']\n",
    "    )\n",
    "    \n",
    "    print(f\"âœ“ Monthly taxi demand records: {len(taxi_demand_monthly):,}\")\n",
    "    print(\"\\nSample:\")\n",
    "    print(taxi_demand_monthly)\n",
    "else:\n",
    "    taxi_demand_monthly = taxi_demand_daily \\\n",
    "        .groupBy(\"year_month\") \\\n",
    "        .agg(\n",
    "            sum(\"pickup_trip_count\").alias(\"pickup_trip_count\"),\n",
    "            sum(\"dropoff_trip_count\").alias(\"dropoff_trip_count\"),\n",
    "            sum(\"total_trip_count\").alias(\"total_trip_count\"),\n",
    "            avg(\"avg_trip_duration\").alias(\"avg_trip_duration\"),\n",
    "            count(\"date\").alias(\"days_in_period\")\n",
    "        ) \\\n",
    "        .withColumn(\"avg_daily_trips\", col(\"total_trip_count\") / col(\"days_in_period\"))\n",
    "    \n",
    "    print(f\"âœ“ Monthly taxi demand records: {taxi_demand_monthly.count():,}\")\n",
    "    taxi_demand_monthly.orderBy(\"year_month\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Build Dimensional Model - Car Price Facts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Creating car price aggregations...\")\n",
    "\n",
    "if LAPTOP_DEPLOYMENT:\n",
    "    # Laptop: Pandas aggregations\n",
    "    \n",
    "    # Daily\n",
    "    car_price_daily = ny_cars_final.groupby('date').agg({\n",
    "        'price': ['median', 'mean', 'count', lambda x: x.quantile(0.25), lambda x: x.quantile(0.75)]\n",
    "    })\n",
    "    car_price_daily.columns = ['median_price', 'avg_price', 'listing_count', 'p25_price', 'p75_price']\n",
    "    car_price_daily = car_price_daily.reset_index()\n",
    "    car_price_daily['date'] = pd.to_datetime(car_price_daily['date'])\n",
    "    car_price_daily['year_month'] = car_price_daily['date'].dt.strftime('%Y-%m')\n",
    "    car_price_daily['year_week'] = car_price_daily['date'].dt.strftime('%Y-%U')\n",
    "    \n",
    "    print(f\"âœ“ Daily car price records: {len(car_price_daily):,}\")\n",
    "    \n",
    "    # Weekly\n",
    "    car_price_weekly = ny_cars_final.groupby('year_week').agg({\n",
    "        'price': ['median', 'mean', 'count']\n",
    "    })\n",
    "    car_price_weekly.columns = ['median_price', 'avg_price', 'listing_count']\n",
    "    car_price_weekly = car_price_weekly.reset_index()\n",
    "    \n",
    "    print(f\"âœ“ Weekly car price records: {len(car_price_weekly):,}\")\n",
    "    \n",
    "    # Monthly\n",
    "    car_price_monthly = ny_cars_final.groupby('year_month').agg({\n",
    "        'price': ['median', 'mean', 'count', lambda x: x.quantile(0.25), lambda x: x.quantile(0.75)]\n",
    "    })\n",
    "    car_price_monthly.columns = ['median_price', 'avg_price', 'listing_count', 'p25_price', 'p75_price']\n",
    "    car_price_monthly = car_price_monthly.reset_index()\n",
    "    \n",
    "    print(f\"âœ“ Monthly car price records: {len(car_price_monthly):,}\")\n",
    "    print(\"\\nSample:\")\n",
    "    print(car_price_monthly)\n",
    "    \n",
    "else:\n",
    "    # Cloud: Spark aggregations\n",
    "    \n",
    "    # Daily\n",
    "    car_price_daily = ny_cars_final \\\n",
    "        .groupBy(\"date\") \\\n",
    "        .agg(\n",
    "            expr(\"percentile_approx(price, 0.5)\").alias(\"median_price\"),\n",
    "            avg(\"price\").alias(\"avg_price\"),\n",
    "            count(\"*\").alias(\"listing_count\"),\n",
    "            expr(\"percentile_approx(price, 0.25)\").alias(\"p25_price\"),\n",
    "            expr(\"percentile_approx(price, 0.75)\").alias(\"p75_price\")\n",
    "        ) \\\n",
    "        .withColumn(\"year_month\", date_format(col(\"date\"), \"yyyy-MM\")) \\\n",
    "        .withColumn(\"year_week\", date_format(col(\"date\"), \"yyyy-ww\"))\n",
    "    \n",
    "    print(f\"âœ“ Daily car price records: {car_price_daily.count():,}\")\n",
    "    \n",
    "    # Weekly\n",
    "    car_price_weekly = ny_cars_final \\\n",
    "        .groupBy(\"year_week\") \\\n",
    "        .agg(\n",
    "            expr(\"percentile_approx(price, 0.5)\").alias(\"median_price\"),\n",
    "            avg(\"price\").alias(\"avg_price\"),\n",
    "            count(\"*\").alias(\"listing_count\")\n",
    "        )\n",
    "    \n",
    "    print(f\"âœ“ Weekly car price records: {car_price_weekly.count():,}\")\n",
    "    \n",
    "    # Monthly\n",
    "    car_price_monthly = ny_cars_final \\\n",
    "        .groupBy(\"year_month\") \\\n",
    "        .agg(\n",
    "            expr(\"percentile_approx(price, 0.5)\").alias(\"median_price\"),\n",
    "            avg(\"price\").alias(\"avg_price\"),\n",
    "            count(\"*\").alias(\"listing_count\"),\n",
    "            expr(\"percentile_approx(price, 0.25)\").alias(\"p25_price\"),\n",
    "            expr(\"percentile_approx(price, 0.75)\").alias(\"p75_price\")\n",
    "        )\n",
    "    \n",
    "    print(f\"âœ“ Monthly car price records: {car_price_monthly.count():,}\")\n",
    "    car_price_monthly.orderBy(\"year_month\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Car price by vehicle type (monthly)\n",
    "print(\"Creating price by vehicle type...\")\n",
    "\n",
    "if LAPTOP_DEPLOYMENT:\n",
    "    car_price_by_type_monthly = ny_cars_final.groupby(['year_month', 'vehicle_type_clean']).agg({\n",
    "        'price': ['median', 'count']\n",
    "    })\n",
    "    car_price_by_type_monthly.columns = ['median_price', 'listing_count']\n",
    "    car_price_by_type_monthly = car_price_by_type_monthly.reset_index()\n",
    "    car_price_by_type_monthly = car_price_by_type_monthly[\n",
    "        car_price_by_type_monthly['listing_count'] >= 10\n",
    "    ]\n",
    "    \n",
    "    print(f\"âœ“ Car price by type (monthly): {len(car_price_by_type_monthly):,}\")\n",
    "else:\n",
    "    car_price_by_type_monthly = ny_cars_final \\\n",
    "        .groupBy(\"year_month\", \"vehicle_type_clean\") \\\n",
    "        .agg(\n",
    "            expr(\"percentile_approx(price, 0.5)\").alias(\"median_price\"),\n",
    "            count(\"*\").alias(\"listing_count\")\n",
    "        ) \\\n",
    "        .filter(col(\"listing_count\") >= 10)\n",
    "    \n",
    "    print(f\"âœ“ Car price by type (monthly): {car_price_by_type_monthly.count():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Car price by age category (monthly)\n",
    "print(\"Creating price by age category...\")\n",
    "\n",
    "if LAPTOP_DEPLOYMENT:\n",
    "    car_price_by_age_monthly = ny_cars_final.groupby(['year_month', 'age_category']).agg({\n",
    "        'price': ['median', 'count']\n",
    "    })\n",
    "    car_price_by_age_monthly.columns = ['median_price', 'listing_count']\n",
    "    car_price_by_age_monthly = car_price_by_age_monthly.reset_index()\n",
    "    car_price_by_age_monthly = car_price_by_age_monthly[\n",
    "        car_price_by_age_monthly['listing_count'] >= 10\n",
    "    ]\n",
    "    \n",
    "    print(f\"âœ“ Car price by age (monthly): {len(car_price_by_age_monthly):,}\")\n",
    "else:\n",
    "    car_price_by_age_monthly = ny_cars_final \\\n",
    "        .groupBy(\"year_month\", \"age_category\") \\\n",
    "        .agg(\n",
    "            expr(\"percentile_approx(price, 0.5)\").alias(\"median_price\"),\n",
    "            count(\"*\").alias(\"listing_count\")\n",
    "        ) \\\n",
    "        .filter(col(\"listing_count\") >= 10)\n",
    "    \n",
    "    print(f\"âœ“ Car price by age (monthly): {car_price_by_age_monthly.count():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Combine Fact Tables (Join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Joining taxi demand and car price facts...\")\n",
    "\n",
    "if LAPTOP_DEPLOYMENT:\n",
    "    # Laptop: Pandas merge\n",
    "    \n",
    "    # Daily\n",
    "    combined_daily = taxi_demand_daily.merge(\n",
    "        car_price_daily[['date', 'median_price', 'avg_price', 'listing_count', 'p25_price', 'p75_price']],\n",
    "        on='date',\n",
    "        how='inner'\n",
    "    ).sort_values('date')\n",
    "    \n",
    "    print(f\"âœ“ Combined daily records: {len(combined_daily):,}\")\n",
    "    \n",
    "    # Weekly\n",
    "    combined_weekly = taxi_demand_weekly.merge(\n",
    "        car_price_weekly,\n",
    "        on='year_week',\n",
    "        how='inner'\n",
    "    ).sort_values('year_week')\n",
    "    \n",
    "    print(f\"âœ“ Combined weekly records: {len(combined_weekly):,}\")\n",
    "    \n",
    "    # Monthly\n",
    "    combined_monthly = taxi_demand_monthly.merge(\n",
    "        car_price_monthly,\n",
    "        on='year_month',\n",
    "        how='inner'\n",
    "    )\n",
    "    combined_monthly = combined_monthly.rename(columns={\n",
    "        'pickup_trip_count': 'monthly_pickups',\n",
    "        'dropoff_trip_count': 'monthly_dropoffs',\n",
    "        'total_trip_count': 'monthly_total_trips'\n",
    "    }).sort_values('year_month')\n",
    "    \n",
    "    print(f\"âœ“ Combined monthly records: {len(combined_monthly):,}\")\n",
    "    print(\"\\nMonthly combined data:\")\n",
    "    print(combined_monthly)\n",
    "    \n",
    "else:\n",
    "    # Cloud: Spark join\n",
    "    \n",
    "    # Daily\n",
    "    combined_daily = taxi_demand_daily \\\n",
    "        .join(car_price_daily, \"date\", \"inner\") \\\n",
    "        .select(\n",
    "            \"date\",\n",
    "            \"year_month\",\n",
    "            \"year_week\",\n",
    "            \"pickup_trip_count\",\n",
    "            \"dropoff_trip_count\",\n",
    "            \"total_trip_count\",\n",
    "            \"avg_trip_duration\",\n",
    "            \"median_price\",\n",
    "            \"avg_price\",\n",
    "            \"listing_count\",\n",
    "            \"p25_price\",\n",
    "            \"p75_price\"\n",
    "        ) \\\n",
    "        .orderBy(\"date\")\n",
    "    \n",
    "    print(f\"âœ“ Combined daily records: {combined_daily.count():,}\")\n",
    "    \n",
    "    # Weekly\n",
    "    combined_weekly = taxi_demand_weekly \\\n",
    "        .join(car_price_weekly, \"year_week\", \"inner\") \\\n",
    "        .orderBy(\"year_week\")\n",
    "    \n",
    "    print(f\"âœ“ Combined weekly records: {combined_weekly.count():,}\")\n",
    "    \n",
    "    # Monthly\n",
    "    combined_monthly = taxi_demand_monthly \\\n",
    "        .join(car_price_monthly, \"year_month\", \"inner\") \\\n",
    "        .select(\n",
    "            \"year_month\",\n",
    "            col(\"pickup_trip_count\").alias(\"monthly_pickups\"),\n",
    "            col(\"dropoff_trip_count\").alias(\"monthly_dropoffs\"),\n",
    "            col(\"total_trip_count\").alias(\"monthly_total_trips\"),\n",
    "            \"avg_daily_trips\",\n",
    "            \"avg_trip_duration\",\n",
    "            \"median_price\",\n",
    "            \"avg_price\",\n",
    "            \"listing_count\",\n",
    "            \"p25_price\",\n",
    "            \"p75_price\"\n",
    "        ) \\\n",
    "        .orderBy(\"year_month\")\n",
    "    \n",
    "    print(f\"âœ“ Combined monthly records: {combined_monthly.count():,}\")\n",
    "    combined_monthly.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Convert to Pandas for Analysis & Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Converting to Pandas for correlation analysis and visualization...\")\n",
    "\n",
    "if LAPTOP_DEPLOYMENT:\n",
    "    # Already in Pandas, just assign\n",
    "    combined_daily_pd = combined_daily\n",
    "    combined_weekly_pd = combined_weekly\n",
    "    combined_monthly_pd = combined_monthly\n",
    "    price_by_type_pd = car_price_by_type_monthly\n",
    "    price_by_age_pd = car_price_by_age_monthly\n",
    "else:\n",
    "    # Convert from Spark to Pandas\n",
    "    combined_daily_pd = combined_daily.toPandas()\n",
    "    combined_weekly_pd = combined_weekly.toPandas()\n",
    "    combined_monthly_pd = combined_monthly.toPandas()\n",
    "    price_by_type_pd = car_price_by_type_monthly.toPandas()\n",
    "    price_by_age_pd = car_price_by_age_monthly.toPandas()\n",
    "\n",
    "print(\"âœ“ Data ready for analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"CORRELATION ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\n1. Daily Granularity:\")\n",
    "print(f\"   Pickup trips vs Median price: {combined_daily_pd['pickup_trip_count'].corr(combined_daily_pd['median_price']):.4f}\")\n",
    "print(f\"   Dropoff trips vs Median price: {combined_daily_pd['dropoff_trip_count'].corr(combined_daily_pd['median_price']):.4f}\")\n",
    "print(f\"   Total trips vs Median price: {combined_daily_pd['total_trip_count'].corr(combined_daily_pd['median_price']):.4f}\")\n",
    "\n",
    "print(\"\\n2. Weekly Granularity:\")\n",
    "print(f\"   Pickup trips vs Median price: {combined_weekly_pd['pickup_trip_count'].corr(combined_weekly_pd['median_price']):.4f}\")\n",
    "print(f\"   Dropoff trips vs Median price: {combined_weekly_pd['dropoff_trip_count'].corr(combined_weekly_pd['median_price']):.4f}\")\n",
    "print(f\"   Total trips vs Median price: {combined_weekly_pd['total_trip_count'].corr(combined_weekly_pd['median_price']):.4f}\")\n",
    "\n",
    "print(\"\\n3. Monthly Granularity:\")\n",
    "print(f\"   Monthly pickups vs Median price: {combined_monthly_pd['monthly_pickups'].corr(combined_monthly_pd['median_price']):.4f}\")\n",
    "print(f\"   Monthly dropoffs vs Median price: {combined_monthly_pd['monthly_dropoffs'].corr(combined_monthly_pd['median_price']):.4f}\")\n",
    "print(f\"   Monthly total trips vs Median price: {combined_monthly_pd['monthly_total_trips'].corr(combined_monthly_pd['median_price']):.4f}\")\n",
    "print(f\"   Avg daily trips vs Median price: {combined_monthly_pd['avg_daily_trips'].corr(combined_monthly_pd['median_price']):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lagged correlation analysis\n",
    "combined_monthly_pd_sorted = combined_monthly_pd.sort_values('year_month').copy()\n",
    "\n",
    "print(\"\\n=== LAGGED CORRELATION ANALYSIS ===\")\n",
    "print(\"(Does taxi demand predict future car prices?)\\n\")\n",
    "\n",
    "for lag in [1, 2, 3]:\n",
    "    combined_monthly_pd_sorted[f'median_price_lag_{lag}'] = combined_monthly_pd_sorted['median_price'].shift(lag)\n",
    "    corr = combined_monthly_pd_sorted['monthly_total_trips'].corr(\n",
    "        combined_monthly_pd_sorted[f'median_price_lag_{lag}']\n",
    "    )\n",
    "    print(f\"Lag {lag} month: Trips vs Price: {corr:.4f}\")\n",
    "\n",
    "print(\"\\n(Does car price predict future taxi demand?)\\n\")\n",
    "for lag in [1, 2, 3]:\n",
    "    combined_monthly_pd_sorted[f'trips_lag_{lag}'] = combined_monthly_pd_sorted['monthly_total_trips'].shift(lag)\n",
    "    corr = combined_monthly_pd_sorted['median_price'].corr(\n",
    "        combined_monthly_pd_sorted[f'trips_lag_{lag}']\n",
    "    )\n",
    "    print(f\"Lag {lag} month: Price vs Trips: {corr:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"DATASET SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nTime period: 2019-2020\")\n",
    "\n",
    "if LAPTOP_DEPLOYMENT:\n",
    "    print(f\"Taxi trips analyzed: {len(taxi_final):,}\")\n",
    "    print(f\"NY car listings analyzed: {len(ny_cars_final):,}\")\n",
    "else:\n",
    "    print(f\"Taxi trips analyzed: {taxi_final.count():,}\")\n",
    "    print(f\"NY car listings analyzed: {ny_cars_final.count():,}\")\n",
    "\n",
    "print(f\"\\nDaily observations: {len(combined_daily_pd):,}\")\n",
    "print(f\"Weekly observations: {len(combined_weekly_pd):,}\")\n",
    "print(f\"Monthly observations: {len(combined_monthly_pd):,}\")\n",
    "\n",
    "print(\"\\n=== KEY INSIGHTS ===\")\n",
    "print(f\"Average daily taxi trips: {combined_monthly_pd['avg_daily_trips'].mean():,.0f}\")\n",
    "print(f\"Average median car price: ${combined_monthly_pd['median_price'].mean():,.2f}\")\n",
    "print(f\"Price range: ${combined_monthly_pd['median_price'].min():,.2f} - ${combined_monthly_pd['median_price'].max():,.2f}\")\n",
    "print(f\"Trip volume range: {combined_monthly_pd['monthly_total_trips'].min():,.0f} - {combined_monthly_pd['monthly_total_trips'].max():,.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Export Combined Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Saving combined datasets...\")\n",
    "\n",
    "if LAPTOP_DEPLOYMENT:\n",
    "    # Laptop: Save as parquet using Pandas\n",
    "    combined_daily_pd.to_parquet('data/combined_daily.parquet', index=False)\n",
    "    combined_weekly_pd.to_parquet('data/combined_weekly.parquet', index=False)\n",
    "    combined_monthly_pd.to_parquet('data/combined_monthly.parquet', index=False)\n",
    "    price_by_type_pd.to_parquet('data/price_by_type_monthly.parquet', index=False)\n",
    "    price_by_age_pd.to_parquet('data/price_by_age_monthly.parquet', index=False)\n",
    "else:\n",
    "    # Cloud: Save using Spark\n",
    "    combined_daily.write.mode('overwrite').parquet('data/combined_daily.parquet')\n",
    "    combined_weekly.write.mode('overwrite').parquet('data/combined_weekly.parquet')\n",
    "    combined_monthly.write.mode('overwrite').parquet('data/combined_monthly.parquet')\n",
    "    car_price_by_type_monthly.write.mode('overwrite').parquet('data/price_by_type_monthly.parquet')\n",
    "    car_price_by_age_monthly.write.mode('overwrite').parquet('data/price_by_age_monthly.parquet')\n",
    "\n",
    "print(\"âœ“ Combined datasets saved to parquet files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Visualizations\n",
    "\n",
    "The visualization code below works the same regardless of deployment mode (since we've converted to Pandas)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph 1: Time Series Overlay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "combined_monthly_pd['year_month_dt'] = pd.to_datetime(combined_monthly_pd['year_month'])\n",
    "\n",
    "# Create figure with secondary y-axis\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add taxi trips trace\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=combined_monthly_pd['year_month_dt'],\n",
    "    y=combined_monthly_pd['monthly_total_trips'],\n",
    "    name='Total Taxi Trips',\n",
    "    line=dict(color='blue', width=2),\n",
    "    yaxis='y'\n",
    "))\n",
    "\n",
    "# Add car price trace\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=combined_monthly_pd['year_month_dt'],\n",
    "    y=combined_monthly_pd['median_price'],\n",
    "    name='Median Car Price',\n",
    "    line=dict(color='red', width=2),\n",
    "    yaxis='y2'\n",
    "))\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title='NYC Taxi Demand vs Used Car Prices Over Time (2019-2020)',\n",
    "    xaxis=dict(title='Month'),\n",
    "    yaxis=dict(\n",
    "        title='Total Taxi Trips',\n",
    "        titlefont=dict(color='blue'),\n",
    "        tickfont=dict(color='blue')\n",
    "    ),\n",
    "    yaxis2=dict(\n",
    "        title='Median Car Price ($)',\n",
    "        titlefont=dict(color='red'),\n",
    "        tickfont=dict(color='red'),\n",
    "        overlaying='y',\n",
    "        side='right'\n",
    "    ),\n",
    "    hovermode='x unified',\n",
    "    height=500\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "print(\"\\nGraph 1: If hypothesis is true, you should see the two lines moving in similar patterns.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph 2: Scatter Plot with Trend Line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create scatter plot\n",
    "fig = px.scatter(\n",
    "    combined_monthly_pd,\n",
    "    x='monthly_total_trips',\n",
    "    y='median_price',\n",
    "    color='year_month_dt',\n",
    "    trendline='ols',\n",
    "    labels={\n",
    "        'monthly_total_trips': 'Total Monthly Taxi Trips',\n",
    "        'median_price': 'Median Car Price ($)',\n",
    "        'year_month_dt': 'Month'\n",
    "    },\n",
    "    title='Correlation: Taxi Trip Volume vs Car Prices',\n",
    "    hover_data=['year_month']\n",
    ")\n",
    "\n",
    "fig.update_layout(height=500)\n",
    "fig.show()\n",
    "\n",
    "corr_val = combined_monthly_pd['monthly_total_trips'].corr(combined_monthly_pd['median_price'])\n",
    "print(f\"\\nGraph 2: Correlation coefficient = {corr_val:.4f}\")\n",
    "print(\"If hypothesis is true, expect positive correlation (upward trend line).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph 3: Correlation Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select numeric columns for correlation\n",
    "corr_cols = [\n",
    "    'monthly_pickups', 'monthly_dropoffs', 'monthly_total_trips',\n",
    "    'avg_daily_trips', 'avg_trip_duration', 'median_price', 'avg_price'\n",
    "]\n",
    "\n",
    "corr_matrix = combined_monthly_pd[corr_cols].corr()\n",
    "\n",
    "# Create heatmap\n",
    "fig = go.Figure(data=go.Heatmap(\n",
    "    z=corr_matrix.values,\n",
    "    x=corr_matrix.columns,\n",
    "    y=corr_matrix.columns,\n",
    "    colorscale='RdBu',\n",
    "    zmid=0,\n",
    "    text=corr_matrix.values,\n",
    "    texttemplate='%{text:.3f}',\n",
    "    textfont={\"size\": 10}\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Correlation Matrix: All Variables',\n",
    "    height=600,\n",
    "    xaxis={'side': 'bottom'}\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "print(\"\\nGraph 3: Look for strong correlations (close to +1 or -1) between trip metrics and price.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph 4: Lagged Correlation Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate lagged correlations\n",
    "lags = list(range(0, 6))\n",
    "trips_to_price_corr = []\n",
    "price_to_trips_corr = []\n",
    "\n",
    "combined_monthly_sorted = combined_monthly_pd.sort_values('year_month')\n",
    "\n",
    "for lag in lags:\n",
    "    if lag == 0:\n",
    "        trips_to_price_corr.append(\n",
    "            combined_monthly_sorted['monthly_total_trips'].corr(combined_monthly_sorted['median_price'])\n",
    "        )\n",
    "        price_to_trips_corr.append(\n",
    "            combined_monthly_sorted['median_price'].corr(combined_monthly_sorted['monthly_total_trips'])\n",
    "        )\n",
    "    else:\n",
    "        trips_to_price_corr.append(\n",
    "            combined_monthly_sorted['monthly_total_trips'].iloc[:-lag].corr(\n",
    "                combined_monthly_sorted['median_price'].iloc[lag:]\n",
    "            )\n",
    "        )\n",
    "        price_to_trips_corr.append(\n",
    "            combined_monthly_sorted['median_price'].iloc[:-lag].corr(\n",
    "                combined_monthly_sorted['monthly_total_trips'].iloc[lag:]\n",
    "            )\n",
    "        )\n",
    "\n",
    "# Plot\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=lags,\n",
    "    y=trips_to_price_corr,\n",
    "    mode='lines+markers',\n",
    "    name='Trips â†’ Price (trips lead)',\n",
    "    line=dict(color='blue', width=2)\n",
    "))\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=lags,\n",
    "    y=price_to_trips_corr,\n",
    "    mode='lines+markers',\n",
    "    name='Price â†’ Trips (price leads)',\n",
    "    line=dict(color='red', width=2)\n",
    "))\n",
    "\n",
    "fig.add_hline(y=0, line_dash=\"dash\", line_color=\"gray\")\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Lagged Correlation: Does One Variable Predict the Other?',\n",
    "    xaxis_title='Lag (months)',\n",
    "    yaxis_title='Correlation Coefficient',\n",
    "    height=500\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "print(\"\\nGraph 4: If one line peaks at lag > 0, that variable predicts the other.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph 5: Vehicle Type & Age Breakdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vehicle type trends\n",
    "top_types = price_by_type_pd.groupby('vehicle_type_clean')['listing_count'].sum().nlargest(6).index\n",
    "price_by_type_filtered = price_by_type_pd[price_by_type_pd['vehicle_type_clean'].isin(top_types)]\n",
    "\n",
    "fig = px.line(\n",
    "    price_by_type_filtered,\n",
    "    x='year_month',\n",
    "    y='median_price',\n",
    "    color='vehicle_type_clean',\n",
    "    title='Median Car Price Over Time by Vehicle Type',\n",
    "    labels={'median_price': 'Median Price ($)', 'year_month': 'Month'},\n",
    "    height=500\n",
    ")\n",
    "\n",
    "fig.update_xaxes(tickangle=45)\n",
    "fig.show()\n",
    "\n",
    "# Vehicle age trends\n",
    "fig = px.line(\n",
    "    price_by_age_pd,\n",
    "    x='year_month',\n",
    "    y='median_price',\n",
    "    color='age_category',\n",
    "    title='Median Car Price Over Time by Vehicle Age',\n",
    "    labels={'median_price': 'Median Price ($)', 'year_month': 'Month'},\n",
    "    height=500\n",
    ")\n",
    "\n",
    "fig.update_xaxes(tickangle=45)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Analysis Complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"  ANALYSIS COMPLETE ({' LAPTOP MODE' if LAPTOP_DEPLOYMENT else 'CLOUD MODE'})\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"1. Review all graphs above\")\n",
    "print(\"2. Look for consistent patterns across multiple visualizations\")\n",
    "print(\"3. Consider alternative explanations for any correlations found\")\n",
    "print(\"4. Focus on pre-COVID data (2019) for cleaner signal\")\n",
    "print(\"5. Be aware of geographic mismatch (NYC taxi vs NY state cars)\")\n",
    "print(\"\\nâš ï¸  Remember: Correlation â‰  Causation\")\n",
    "print(\"\\nâœ“ Output files saved to data/ directory\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
