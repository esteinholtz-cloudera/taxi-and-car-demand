{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# NYC Taxi Demand vs Used Car Price Analysis (Hybrid: Laptop/Cloud)\n",
        "## Hypothesis: Taxi demand correlates with used car prices in New York areas\n",
        "\n",
        "This notebook analyzes the correlation between NYC taxi trip density (2019-2020) and Craigslist used car prices in NY regions.\n",
        "\n",
        "**Deployment Mode**: Toggle `LAPTOP_DEPLOYMENT` flag below to switch between:\n",
        "- `True`: Pandas-only (Mac laptop friendly, < 16GB RAM recommended)\n",
        "- `False`: Spark + Pandas (Cloudera AI / cluster deployment)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîß DEPLOYMENT CONFIGURATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# TOGGLE THIS FLAG TO SWITCH DEPLOYMENT MODES\n",
        "# ============================================================================\n",
        "\n",
        "\n",
        "LAPTOP_DEPLOYMENT = True  # Set to False for Spark/Cloud deployment\n",
        "# ============================================================================\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"  DEPLOYMENT MODE: {'LAPTOP (Pandas)' if LAPTOP_DEPLOYMENT else 'CLOUD (Spark)'}\")\n",
        "print(f\"{'='*60}\\n\")\n",
        "\n",
        "if LAPTOP_DEPLOYMENT:\n",
        "    print(\"‚úì Running in Laptop mode (Pandas-only)\")\n",
        "    print(\"  - No Spark required\")\n",
        "    print(\"  - Lighter memory footprint\")\n",
        "    print(\"  - Good for datasets < 10GB\")\n",
        "else:\n",
        "    print(\"‚úì Running in Cloud mode (Spark + Pandas)\")\n",
        "    print(\"  - Distributed processing enabled\")\n",
        "    print(\"  - Scalable for large datasets\")\n",
        "    print(\"  - Optimized for Cloudera AI\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Package Installation\n",
        "\n",
        "**üí° Recommended Alternative:** Use `uv sync` before opening the notebook for faster, reproducible installs!\n",
        "\n",
        "```bash\n",
        "# Laptop mode: uv sync --extra laptop --extra dev\n",
        "# Cloud mode:  uv sync --extra cloud --extra dev\n",
        "# See UV_SETUP.md for details\n",
        "```\n",
        "\n",
        "If you've already run `uv sync`, you can **skip this cell** entirely!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# INSTALLATION METHOD: Choose pip or uv\n",
        "# ============================================================================\n",
        "USE_UV = False  # Set to True to use uv (faster), False for pip\n",
        "# ============================================================================\n",
        "\n",
        "import sys\n",
        "\n",
        "if USE_UV:\n",
        "    # Using uv (faster package installer)\n",
        "    # Install uv first if not already installed: curl -LsSf https://astral.sh/uv/install.sh | sh\n",
        "    print(\"üì¶ Installing packages with uv (fast mode)...\")\n",
        "    \n",
        "    if LAPTOP_DEPLOYMENT:\n",
        "        # Laptop mode packages\n",
        "        !uv pip install --system kaggle pandas numpy matplotlib seaborn plotly pyarrow fastparquet\n",
        "    else:\n",
        "        # Cloud mode packages\n",
        "        !uv pip install --system kaggle opendatasets pandas numpy matplotlib seaborn plotly\n",
        "else:\n",
        "    # Using standard pip\n",
        "    print(\"üì¶ Installing packages with pip...\")\n",
        "    \n",
        "    if LAPTOP_DEPLOYMENT:\n",
        "        # Laptop mode packages\n",
        "        !pip install kaggle pandas numpy matplotlib seaborn plotly pyarrow fastparquet -q\n",
        "    else:\n",
        "        # Cloud mode packages\n",
        "        !pip install kaggle opendatasets pandas numpy matplotlib seaborn plotly -q\n",
        "\n",
        "print(\"‚úì Packages installed successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Import Libraries and Initialize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from datetime import datetime\n",
        "\n",
        "if LAPTOP_DEPLOYMENT:\n",
        "    # Laptop mode: Pandas only\n",
        "    print(\"Initializing Pandas environment...\")\n",
        "    # Optimize pandas for larger datasets\n",
        "    pd.options.mode.chained_assignment = None\n",
        "    print(f\"‚úì Pandas version: {pd.__version__}\")\n",
        "else:\n",
        "    # Cloud mode: Import and initialize Spark\n",
        "    print(\"Initializing Spark environment...\")\n",
        "    from pyspark.sql import SparkSession\n",
        "    from pyspark.sql.functions import *\n",
        "    from pyspark.sql.types import *\n",
        "    from pyspark.sql.window import Window\n",
        "    \n",
        "    spark = SparkSession.builder \\\n",
        "        .appName(\"TaxiCarPriceCorrelation\") \\\n",
        "        .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
        "        .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
        "        .getOrCreate()\n",
        "    \n",
        "    print(f\"‚úì Spark version: {spark.version}\")\n",
        "    print(f\"‚úì Pandas version: {pd.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Download Datasets\n",
        "\n",
        "**Note**: You need Kaggle API credentials:\n",
        "1. Go to Kaggle ‚Üí Account ‚Üí Create New API Token\n",
        "2. Place `kaggle.json` in `~/.kaggle/` directory\n",
        "3. Run: `chmod 600 ~/.kaggle/kaggle.json`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create data directory\n",
        "os.makedirs('data', exist_ok=True)\n",
        "\n",
        "# Download Craigslist cars dataset\n",
        "print(\"Downloading Craigslist cars dataset...\")\n",
        "!kaggle datasets download -d austinreese/craigslist-carstrucks-data -p ./data --unzip\n",
        "\n",
        "# Download NYC taxi dataset\n",
        "print(\"\\nDownloading NYC Yellow Taxi dataset...\")\n",
        "!kaggle datasets download -d microize/newyork-yellow-taxi-trip-data-2020-2019 -p ./data --unzip\n",
        "\n",
        "print(\"\\n‚úì Download complete!\")\n",
        "!ls -lh data/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Loading Craigslist dataset...\")\n",
        "\n",
        "if LAPTOP_DEPLOYMENT:\n",
        "    # Laptop: Use Pandas with optimized dtypes\n",
        "    craigslist_df = pd.read_csv(\n",
        "        \"data/vehicles.csv\",\n",
        "        low_memory=False\n",
        "    )\n",
        "    print(f\"‚úì Loaded {len(craigslist_df):,} records\")\n",
        "    print(f\"\\nColumns: {list(craigslist_df.columns)}\")\n",
        "    print(f\"\\nSample data:\")\n",
        "    print(craigslist_df.head())\n",
        "else:\n",
        "    # Cloud: Use Spark\n",
        "    craigslist_df = spark.read.csv(\n",
        "        \"data/vehicles.csv\",\n",
        "        header=True,\n",
        "        inferSchema=True\n",
        "    )\n",
        "    print(\"Craigslist Dataset Schema:\")\n",
        "    craigslist_df.printSchema()\n",
        "    print(f\"\\n‚úì Total records: {craigslist_df.count():,}\")\n",
        "    print(\"\\nSample data:\")\n",
        "    craigslist_df.show(5, truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check available states\n",
        "print(\"States in Craigslist dataset:\")\n",
        "\n",
        "if LAPTOP_DEPLOYMENT:\n",
        "    state_counts = craigslist_df['state'].value_counts().head(10)\n",
        "    print(state_counts)\n",
        "else:\n",
        "    craigslist_df.groupBy(\"state\").count().orderBy(desc(\"count\")).show(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4.5 Explore Craigslist Data: Ranges and Distributions\n",
        "\n",
        "Comprehensive exploration of the Craigslist dataset to understand data quality, ranges, and distributions.\n",
        "\n",
        "**Note:** Taxi data exploration will occur after taxi data is loaded (Section 4.6)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"CRAIGSLIST DATASET EXPLORATION\")\n",
        "print(\"=\"*80)\n",
        "print(\"‚ö†Ô∏è  Note: For large Spark datasets, some operations use sampling to avoid memory/shuffle issues\")\n",
        "print(\"\")\n",
        "\n",
        "if LAPTOP_DEPLOYMENT:\n",
        "    # Pandas exploration\n",
        "    df = craigslist_df\n",
        "    \n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(\"1. DATASET OVERVIEW\")\n",
        "    print(f\"{'='*80}\")\n",
        "    print(f\"Total records: {len(df):,}\")\n",
        "    print(f\"Total columns: {len(df.columns)}\")\n",
        "    print(f\"\\nColumns: {', '.join(df.columns)}\")\n",
        "    \n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(\"2. DATA TYPES AND MISSING VALUES\")\n",
        "    print(f\"{'='*80}\")\n",
        "    info_df = pd.DataFrame({\n",
        "        'Column': df.columns,\n",
        "        'Data Type': df.dtypes,\n",
        "        'Non-Null Count': df.count(),\n",
        "        'Null Count': df.isnull().sum(),\n",
        "        'Null %': (df.isnull().sum() / len(df) * 100).round(2)\n",
        "    })\n",
        "    print(info_df.to_string(index=False))\n",
        "    \n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(\"3. NUMERIC FIELDS - RANGES AND DISTRIBUTIONS\")\n",
        "    print(f\"{'='*80}\")\n",
        "    \n",
        "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
        "    for col in numeric_cols:\n",
        "        if df[col].notna().sum() > 0:\n",
        "            print(f\"\\n{col.upper()}:\")\n",
        "            print(f\"  Count: {df[col].notna().sum():,}\")\n",
        "            print(f\"  Min: {df[col].min():,.2f}\")\n",
        "            print(f\"  25th percentile: {df[col].quantile(0.25):,.2f}\")\n",
        "            print(f\"  Median (50th): {df[col].median():,.2f}\")\n",
        "            print(f\"  Mean: {df[col].mean():,.2f}\")\n",
        "            print(f\"  75th percentile: {df[col].quantile(0.75):,.2f}\")\n",
        "            print(f\"  Max: {df[col].max():,.2f}\")\n",
        "            print(f\"  Std Dev: {df[col].std():,.2f}\")\n",
        "            print(f\"  Range: {df[col].max() - df[col].min():,.2f}\")\n",
        "    \n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(\"4. CATEGORICAL FIELDS - VALUE COUNTS\")\n",
        "    print(f\"{'='*80}\")\n",
        "    \n",
        "    categorical_cols = df.select_dtypes(include=['object']).columns\n",
        "    for col in categorical_cols[:10]:  # Limit to first 10 to avoid too much output\n",
        "        if df[col].notna().sum() > 0:\n",
        "            n_unique = df[col].nunique()\n",
        "            print(f\"\\n{col.upper()} ({n_unique:,} unique values):\")\n",
        "            if n_unique <= 20:  # Show full distribution if few values\n",
        "                value_counts = df[col].value_counts().head(20)\n",
        "                for val, count in value_counts.items():\n",
        "                    pct = (count / len(df) * 100)\n",
        "                    print(f\"  {val}: {count:,} ({pct:.1f}%)\")\n",
        "            else:  # Show top 10 for many values\n",
        "                value_counts = df[col].value_counts().head(10)\n",
        "                print(f\"  Top 10 values:\")\n",
        "                for val, count in value_counts.items():\n",
        "                    pct = (count / len(df) * 100)\n",
        "                    print(f\"    {val}: {count:,} ({pct:.1f}%)\")\n",
        "    \n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(\"5. KEY FIELD: PRICE DISTRIBUTION\")\n",
        "    print(f\"{'='*80}\")\n",
        "    if 'price' in df.columns:\n",
        "        price_valid = df[df['price'].notna() & (df['price'] > 0)]\n",
        "        print(f\"Valid prices (> 0): {len(price_valid):,} ({len(price_valid)/len(df)*100:.1f}%)\")\n",
        "        print(f\"\\nPrice percentiles:\")\n",
        "        for p in [1, 5, 10, 25, 50, 75, 90, 95, 99]:\n",
        "            print(f\"  {p}th: ${price_valid['price'].quantile(p/100):,.0f}\")\n",
        "    \n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(\"6. KEY FIELD: YEAR DISTRIBUTION\")\n",
        "    print(f\"{'='*80}\")\n",
        "    if 'year' in df.columns:\n",
        "        year_valid = df[df['year'].notna()]\n",
        "        print(f\"Valid years: {len(year_valid):,} ({len(year_valid)/len(df)*100:.1f}%)\")\n",
        "        print(f\"Year range: {year_valid['year'].min():.0f} - {year_valid['year'].max():.0f}\")\n",
        "        print(f\"\\nYear distribution (top 10):\")\n",
        "        print(year_valid['year'].value_counts().head(10))\n",
        "    \n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(\"7. GEOGRAPHIC DISTRIBUTION\")\n",
        "    print(f\"{'='*80}\")\n",
        "    if 'state' in df.columns:\n",
        "        print(\"Top 15 states by listing count:\")\n",
        "        print(df['state'].value_counts().head(15))\n",
        "\n",
        "else:\n",
        "    # Spark exploration - with error handling for shuffle/memory issues\n",
        "    from pyspark.sql.functions import col, count, when, isnan, avg, stddev, min as spark_min, max as spark_max, percentile_approx\n",
        "    \n",
        "    df = craigslist_df\n",
        "    \n",
        "    # Cache to reduce recomputation\n",
        "    print(\"Caching dataset for exploration...\")\n",
        "    df.cache()\n",
        "    \n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(\"1. DATASET OVERVIEW\")\n",
        "    print(f\"{'='*80}\")\n",
        "    try:\n",
        "        total_records = df.count()\n",
        "        print(f\"Total records: {total_records:,}\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è  Could not count records: {type(e).__name__}\")\n",
        "        print(\"Large dataset - using sample for exploration\")\n",
        "        total_records = None\n",
        "    \n",
        "    print(f\"Total columns: {len(df.columns)}\")\n",
        "    print(f\"\\nColumns: {', '.join(df.columns)}\")\n",
        "    \n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(\"2. SCHEMA AND DATA TYPES\")\n",
        "    print(f\"{'='*80}\")\n",
        "    df.printSchema()\n",
        "    \n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(\"3. NULL/MISSING VALUE ANALYSIS\")\n",
        "    print(f\"{'='*80}\")\n",
        "    try:\n",
        "        if total_records is None:\n",
        "            total_count = df.count()\n",
        "        else:\n",
        "            total_count = total_records\n",
        "            \n",
        "        null_counts = df.select([\n",
        "            count(when(col(c).isNull(), c)).alias(c) \n",
        "            for c in df.columns\n",
        "        ]).collect()[0].asDict()\n",
        "        \n",
        "        print(f\"{'Column':<25} {'Null Count':<15} {'Null %':<10}\")\n",
        "        print(\"-\" * 50)\n",
        "        for column in df.columns:\n",
        "            null_count = null_counts[column]\n",
        "            null_pct = (null_count / total_count * 100)\n",
        "            print(f\"{column:<25} {null_count:<15,} {null_pct:<10.2f}\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è  Null analysis failed: {type(e).__name__}\")\n",
        "        print(\"Skipping detailed null analysis\")\n",
        "    \n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(\"4. NUMERIC FIELDS - SUMMARY STATISTICS\")\n",
        "    print(f\"{'='*80}\")\n",
        "    numeric_cols = [field.name for field in df.schema.fields \n",
        "                   if str(field.dataType) in ['IntegerType', 'LongType', 'FloatType', 'DoubleType']]\n",
        "    \n",
        "    if numeric_cols:\n",
        "        try:\n",
        "            df.select(numeric_cols).summary(\"count\", \"mean\", \"stddev\", \"min\", \"50%\", \"max\").show(truncate=False)\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è  Summary statistics failed: {type(e).__name__}\")\n",
        "            print(\"Trying with sample...\")\n",
        "            try:\n",
        "                df_sample = df.select(numeric_cols).sample(fraction=0.1).toPandas()\n",
        "                print(df_sample.describe())\n",
        "            except:\n",
        "                print(\"‚ö†Ô∏è  Could not generate statistics. Use LAPTOP_DEPLOYMENT=True instead.\")\n",
        "    \n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(\"5. CATEGORICAL FIELDS - VALUE COUNTS (Using Sample to Avoid Shuffle Issues)\")\n",
        "    print(f\"{'='*80}\")\n",
        "    print(\"‚ÑπÔ∏è  Converting 10% sample to Pandas for robust aggregation\\n\")\n",
        "    \n",
        "    try:\n",
        "        # Sample and convert to pandas to avoid Spark shuffle issues\n",
        "        df_sample = df.sample(fraction=0.1, seed=42).toPandas()\n",
        "        \n",
        "        categorical_cols = ['state', 'manufacturer', 'type', 'condition', 'fuel', 'transmission']\n",
        "        for col_name in categorical_cols:\n",
        "            if col_name in df_sample.columns and df_sample[col_name].notna().sum() > 0:\n",
        "                print(f\"\\n{col_name.upper()} distribution (top 10 from 10% sample):\")\n",
        "                value_counts = df_sample[col_name].value_counts().head(10)\n",
        "                for val, count in value_counts.items():\n",
        "                    estimated_total = count * 10  # Rough estimate\n",
        "                    print(f\"  {val}: ~{estimated_total:,} (sample: {count:,})\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è  Could not generate value counts: {type(e).__name__}\")\n",
        "        print(\"This is likely due to Spark shuffle issues with large datasets.\")\n",
        "        print(\"Recommendation: Use LAPTOP_DEPLOYMENT=True for better data exploration\")\n",
        "    \n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(\"6. KEY FIELD: PRICE DISTRIBUTION\")\n",
        "    print(f\"{'='*80}\")\n",
        "    if 'price' in df.columns:\n",
        "        try:\n",
        "            price_stats = df.filter(col('price').isNotNull() & (col('price') > 0)).select(\n",
        "                count('price').alias('count'),\n",
        "                spark_min('price').alias('min'),\n",
        "                percentile_approx('price', 0.25).alias('p25'),\n",
        "                percentile_approx('price', 0.50).alias('median'),\n",
        "                percentile_approx('price', 0.75).alias('p75'),\n",
        "                spark_max('price').alias('max'),\n",
        "                avg('price').alias('mean'),\n",
        "                stddev('price').alias('stddev')\n",
        "            ).collect()[0]\n",
        "            \n",
        "            print(f\"Valid prices (> 0): {price_stats['count']:,}\")\n",
        "            print(f\"Min: ${price_stats['min']:,.0f}\")\n",
        "            print(f\"25th percentile: ${price_stats['p25']:,.0f}\")\n",
        "            print(f\"Median: ${price_stats['median']:,.0f}\")\n",
        "            print(f\"Mean: ${price_stats['mean']:,.0f}\")\n",
        "            print(f\"75th percentile: ${price_stats['p75']:,.0f}\")\n",
        "            print(f\"Max: ${price_stats['max']:,.0f}\")\n",
        "            print(f\"Std Dev: ${price_stats['stddev']:,.0f}\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è  Price statistics failed: {type(e).__name__}\")\n",
        "            print(\"Try using LAPTOP_DEPLOYMENT=True for better exploration\")\n",
        "    \n",
        "    # Clean up cache\n",
        "    df.unpersist()\n",
        "\n",
        "print(\"\\n‚úì Craigslist data exploration complete!\")\n",
        "print(\"\\n‚ÑπÔ∏è  Taxi data exploration will be performed after taxi data is loaded (Section 4.6)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.5.1 Craigslist Visual Distributions\n",
        "\n",
        "Generate quick visualizations for key Craigslist fields to understand distributions visually.\n",
        "\n",
        "**Note:** Taxi visualizations will be in Section 4.6 after taxi data is loaded."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Craigslist visualizations\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Set style\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (15, 10)\n",
        "\n",
        "print(\"Generating Craigslist distribution plots...\")\n",
        "\n",
        "# Convert to pandas if using Spark\n",
        "if LAPTOP_DEPLOYMENT:\n",
        "    cars_pd = craigslist_df\n",
        "else:\n",
        "    # Sample for visualization to avoid memory issues\n",
        "    try:\n",
        "        print(\"Sampling Craigslist data for visualization (10%)...\")\n",
        "        cars_pd = craigslist_df.sample(fraction=0.1, seed=42).toPandas()\n",
        "        print(f\"‚úì Sampled {len(cars_pd):,} car records\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è  Sampling failed: {type(e).__name__}\")\n",
        "        print(\"Visualization may not be available. Try LAPTOP_DEPLOYMENT=True\")\n",
        "        cars_pd = pd.DataFrame()\n",
        "\n",
        "# Skip visualization if data is empty\n",
        "if len(cars_pd) == 0:\n",
        "    print(\"‚ö†Ô∏è  Cannot generate visualizations - data sampling failed\")\n",
        "    print(\"\\nüí° SOLUTION: Set LAPTOP_DEPLOYMENT=True at the top of the notebook\")\n",
        "    print(\"This avoids Spark shuffle issues for data exploration\")\n",
        "else:\n",
        "    # Create subplots (2x3 grid for 6 Craigslist plots)\n",
        "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
        "    fig.suptitle('Craigslist Data Distribution Analysis', fontsize=16, fontweight='bold')\n",
        "\n",
        "# 1. Price distribution (Craigslist)\n",
        "if 'price' in cars_pd.columns:\n",
        "    ax = axes[0, 0]\n",
        "    price_clean = cars_pd[(cars_pd['price'] > 0) & (cars_pd['price'] < 100000)]\n",
        "    ax.hist(price_clean['price'], bins=50, edgecolor='black', alpha=0.7)\n",
        "    ax.set_xlabel('Price ($)')\n",
        "    ax.set_ylabel('Frequency')\n",
        "    ax.set_title('Craigslist: Price Distribution')\n",
        "    ax.axvline(price_clean['price'].median(), color='red', linestyle='--', label=f'Median: ${price_clean[\"price\"].median():,.0f}')\n",
        "    ax.legend()\n",
        "\n",
        "# 2. Year distribution (Craigslist)\n",
        "if 'year' in cars_pd.columns:\n",
        "    ax = axes[0, 1]\n",
        "    year_clean = cars_pd[cars_pd['year'].notna() & (cars_pd['year'] >= 1990)]\n",
        "    ax.hist(year_clean['year'], bins=35, edgecolor='black', alpha=0.7)\n",
        "    ax.set_xlabel('Year')\n",
        "    ax.set_ylabel('Frequency')\n",
        "    ax.set_title('Craigslist: Vehicle Year Distribution')\n",
        "\n",
        "# 3. Odometer distribution (Craigslist)\n",
        "if 'odometer' in cars_pd.columns:\n",
        "    ax = axes[0, 2]\n",
        "    odometer_clean = cars_pd[(cars_pd['odometer'] > 0) & (cars_pd['odometer'] < 300000)]\n",
        "    ax.hist(odometer_clean['odometer'], bins=50, edgecolor='black', alpha=0.7)\n",
        "    ax.set_xlabel('Odometer (miles)')\n",
        "    ax.set_ylabel('Frequency')\n",
        "    ax.set_title('Craigslist: Odometer Distribution')\n",
        "\n",
        "# 4. State distribution (Craigslist - top 10)\n",
        "if 'state' in cars_pd.columns:\n",
        "    ax = axes[1, 0]\n",
        "    top_states = cars_pd['state'].value_counts().head(10)\n",
        "    ax.barh(range(len(top_states)), top_states.values)\n",
        "    ax.set_yticks(range(len(top_states)))\n",
        "    ax.set_yticklabels(top_states.index)\n",
        "    ax.set_xlabel('Count')\n",
        "    ax.set_title('Craigslist: Top 10 States')\n",
        "    ax.invert_yaxis()\n",
        "\n",
        "# 5. Vehicle type distribution (Craigslist)\n",
        "if 'type' in cars_pd.columns:\n",
        "    ax = axes[1, 1]\n",
        "    type_counts = cars_pd['type'].value_counts().head(10)\n",
        "    ax.bar(range(len(type_counts)), type_counts.values)\n",
        "    ax.set_xticks(range(len(type_counts)))\n",
        "    ax.set_xticklabels(type_counts.index, rotation=45, ha='right')\n",
        "    ax.set_ylabel('Count')\n",
        "    ax.set_title('Craigslist: Vehicle Type Distribution')\n",
        "\n",
        "# 6. Manufacturer distribution (Craigslist - top 10)\n",
        "if 'manufacturer' in cars_pd.columns:\n",
        "    ax = axes[1, 2]\n",
        "    mfr_counts = cars_pd['manufacturer'].value_counts().head(10)\n",
        "    ax.barh(range(len(mfr_counts)), mfr_counts.values)\n",
        "    ax.set_yticks(range(len(mfr_counts)))\n",
        "    ax.set_yticklabels(mfr_counts.index)\n",
        "    ax.set_xlabel('Count')\n",
        "    ax.set_title('Craigslist: Top 10 Manufacturers')\n",
        "    ax.invert_yaxis()\n",
        "\n",
        "# 7. Trip distance distribution (Taxi)\n",
        "trip_dist_cols = [col for col in taxi_pd.columns if 'distance' in col.lower()]\n",
        "if trip_dist_cols:\n",
        "    ax = axes[2, 0]\n",
        "    col_name = trip_dist_cols[0]\n",
        "    dist_clean = taxi_pd[(taxi_pd[col_name] > 0) & (taxi_pd[col_name] < 50)]\n",
        "    ax.hist(dist_clean[col_name], bins=50, edgecolor='black', alpha=0.7)\n",
        "    ax.set_xlabel('Trip Distance (miles)')\n",
        "    ax.set_ylabel('Frequency')\n",
        "    ax.set_title(f'Taxi: {col_name} Distribution')\n",
        "    ax.axvline(dist_clean[col_name].median(), color='red', linestyle='--', label=f'Median: {dist_clean[col_name].median():.2f}')\n",
        "    ax.legend()\n",
        "else:\n",
        "    axes[2, 0].text(0.5, 0.5, 'Trip distance\\ncolumn not found', ha='center', va='center', fontsize=12)\n",
        "    axes[2, 0].axis('off')\n",
        "\n",
        "# 8. Passenger count distribution (Taxi)\n",
        "pass_cols = [col for col in taxi_pd.columns if 'passenger' in col.lower()]\n",
        "if pass_cols:\n",
        "    ax = axes[2, 1]\n",
        "    col_name = pass_cols[0]\n",
        "    pass_counts = taxi_pd[col_name].value_counts().sort_index().head(10)\n",
        "    ax.bar(pass_counts.index, pass_counts.values)\n",
        "    ax.set_xlabel('Passenger Count')\n",
        "    ax.set_ylabel('Frequency')\n",
        "    ax.set_title(f'Taxi: {col_name} Distribution')\n",
        "else:\n",
        "    axes[2, 1].text(0.5, 0.5, 'Passenger count\\ncolumn not found', ha='center', va='center', fontsize=12)\n",
        "    axes[2, 1].axis('off')\n",
        "\n",
        "# 9. Trip amount distribution (Taxi)\n",
        "amount_cols = [col for col in taxi_pd.columns if 'amount' in col.lower() or 'fare' in col.lower()]\n",
        "if amount_cols:\n",
        "    ax = axes[2, 2]\n",
        "    col_name = amount_cols[0]\n",
        "    amount_clean = taxi_pd[(taxi_pd[col_name] > 0) & (taxi_pd[col_name] < 100)]\n",
        "    ax.hist(amount_clean[col_name], bins=50, edgecolor='black', alpha=0.7)\n",
        "    ax.set_xlabel('Amount ($)')\n",
        "    ax.set_ylabel('Frequency')\n",
        "    ax.set_title(f'Taxi: {col_name} Distribution')\n",
        "    ax.axvline(amount_clean[col_name].median(), color='red', linestyle='--', label=f'Median: ${amount_clean[col_name].median():.2f}')\n",
        "    ax.legend()\n",
        "else:\n",
        "    axes[2, 2].text(0.5, 0.5, 'Amount/fare\\ncolumn not found', ha='center', va='center', fontsize=12)\n",
        "    axes[2, 2].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    print(\"‚úì Distribution plots generated!\")\n",
        "    print(\"\\nüìä Key Observations to Note:\")\n",
        "    print(\"  - Check for outliers in price, odometer, and trip distance\")\n",
        "    print(\"  - Verify year distribution aligns with 2019-2020 for analysis\")\n",
        "    print(\"  - Note dominant states, vehicle types, and manufacturers\")\n",
        "    print(\"  - Assess data quality from distributions\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ‚ö†Ô∏è Troubleshooting: If You See Spark Shuffle Errors\n",
        "\n",
        "**Error:** `Py4JJavaError: Stream is corrupted` or `FetchFailedException`\n",
        "\n",
        "**Root Cause:** Spark shuffle operations fail with large datasets due to memory pressure or corrupted shuffle files.\n",
        "\n",
        "**Solutions:**\n",
        "\n",
        "**Option 1 (Recommended): Use Laptop Mode**\n",
        "```python\n",
        "# At the top of the notebook, set:\n",
        "LAPTOP_DEPLOYMENT = True\n",
        "```\n",
        "This uses Pandas instead of Spark and avoids shuffle issues entirely.\n",
        "\n",
        "**Option 2: Increase Spark Memory**\n",
        "```python\n",
        "# Before creating SparkSession, add:\n",
        "spark = SparkSession.builder \\\n",
        "    .config(\"spark.driver.memory\", \"8g\") \\\n",
        "    .config(\"spark.executor.memory\", \"8g\") \\\n",
        "    .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
        "    .getOrCreate()\n",
        "```\n",
        "\n",
        "**Option 3: Skip Exploration Cell**\n",
        "The exploration cell (section 4.5) is optional. You can skip it and proceed directly to data preparation (section 5).\n",
        "\n",
        "**Option 4: Use Smaller Sample**\n",
        "Reduce the sample fraction in visualization cells if errors persist."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\nLoading NYC Taxi dataset...\")\n",
        "\n",
        "# Detect file format\n",
        "taxi_files = !ls data/*.parquet data/*.csv 2>/dev/null || echo \"no_files\"\n",
        "is_parquet = any('.parquet' in f for f in taxi_files)\n",
        "\n",
        "if LAPTOP_DEPLOYMENT:\n",
        "    # Laptop: Use Pandas with CSV files (avoids parquet issues)\n",
        "    import glob\n",
        "    \n",
        "    # Find CSV files\n",
        "    csv_files = glob.glob(\"data/yellow_tripdata_*.csv\")\n",
        "    \n",
        "    if csv_files:\n",
        "        print(f\"Found {len(csv_files)} CSV files. Loading... (this may take a few minutes)\")\n",
        "        # Load first file to get schema, then load all\n",
        "        taxi_dfs = []\n",
        "        for i, csv_file in enumerate(csv_files, 1):\n",
        "            print(f\"  Loading file {i}/{len(csv_files)}: {csv_file.split('/')[-1]}\")\n",
        "            df_chunk = pd.read_csv(csv_file, low_memory=False)\n",
        "            taxi_dfs.append(df_chunk)\n",
        "        \n",
        "        taxi_df = pd.concat(taxi_dfs, ignore_index=True)\n",
        "        print(f\"\\n‚úì Loaded {len(taxi_df):,} total records from {len(csv_files)} files\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è  No CSV files found. Check data directory.\")\n",
        "        taxi_df = pd.DataFrame()  # Empty dataframe\n",
        "    \n",
        "    if len(taxi_df) > 0:\n",
        "        print(f\"\\nColumns: {list(taxi_df.columns)}\")\n",
        "        print(f\"\\nSample data:\")\n",
        "        print(taxi_df.head())\n",
        "        \n",
        "else:\n",
        "    # Cloud: Use Spark\n",
        "    taxi_path = \"data/yellow_tripdata_*.csv\"  # Load CSV files\n",
        "    \n",
        "    taxi_df = spark.read.csv(taxi_path, header=True, inferSchema=True)\n",
        "    \n",
        "    print(\"NYC Taxi Dataset Schema:\")\n",
        "    taxi_df.printSchema()\n",
        "    print(f\"\\n‚úì Total records: {taxi_df.count():,}\")\n",
        "    print(\"\\nSample data (first 5 rows):\")\n",
        "    taxi_df.show(5, truncate=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4.6 Explore Taxi Data: Ranges and Distributions\n",
        "\n",
        "Now that taxi data is loaded, explore its quality, ranges, and distributions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"TAXI DATASET EXPLORATION\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "if LAPTOP_DEPLOYMENT:\n",
        "    # Pandas exploration for taxi data\n",
        "    df_taxi = taxi_df\n",
        "    \n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(\"1. DATASET OVERVIEW\")\n",
        "    print(f\"{'='*80}\")\n",
        "    print(f\"Total records: {len(df_taxi):,}\")\n",
        "    print(f\"Total columns: {len(df_taxi.columns)}\")\n",
        "    print(f\"\\nColumns: {', '.join(df_taxi.columns)}\")\n",
        "    \n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(\"2. DATA TYPES AND MISSING VALUES\")\n",
        "    print(f\"{'='*80}\")\n",
        "    info_df = pd.DataFrame({\n",
        "        'Column': df_taxi.columns,\n",
        "        'Data Type': df_taxi.dtypes,\n",
        "        'Non-Null Count': df_taxi.count(),\n",
        "        'Null Count': df_taxi.isnull().sum(),\n",
        "        'Null %': (df_taxi.isnull().sum() / len(df_taxi) * 100).round(2)\n",
        "    })\n",
        "    print(info_df.to_string(index=False))\n",
        "    \n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(\"3. NUMERIC FIELDS - SUMMARY STATISTICS\")\n",
        "    print(f\"{'='*80}\")\n",
        "    print(df_taxi.describe())\n",
        "    \n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(\"4. KEY FIELDS - DETAILED RANGES\")\n",
        "    print(f\"{'='*80}\")\n",
        "    \n",
        "    key_numeric_fields = [col for col in df_taxi.columns \n",
        "                         if df_taxi[col].dtype in ['int64', 'float64'] \n",
        "                         and col not in ['VendorID']][:10]\n",
        "    \n",
        "    for col in key_numeric_fields:\n",
        "        if df_taxi[col].notna().sum() > 0:\n",
        "            print(f\"\\n{col}:\")\n",
        "            print(f\"  Min: {df_taxi[col].min()}\")\n",
        "            print(f\"  25%: {df_taxi[col].quantile(0.25)}\")\n",
        "            print(f\"  Median: {df_taxi[col].median()}\")\n",
        "            print(f\"  75%: {df_taxi[col].quantile(0.75)}\")\n",
        "            print(f\"  Max: {df_taxi[col].max()}\")\n",
        "            print(f\"  Mean: {df_taxi[col].mean():.2f}\")\n",
        "\n",
        "else:\n",
        "    # Spark exploration for taxi data - with error handling\n",
        "    df_taxi = taxi_df\n",
        "    \n",
        "    df_taxi.cache()\n",
        "    \n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(\"1. DATASET OVERVIEW\")\n",
        "    print(f\"{'='*80}\")\n",
        "    try:\n",
        "        taxi_count = df_taxi.count()\n",
        "        print(f\"Total records: {taxi_count:,}\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è  Could not count records: {type(e).__name__}\")\n",
        "        print(\"Using sampling for exploration...\")\n",
        "        taxi_count = None\n",
        "    \n",
        "    print(f\"Total columns: {len(df_taxi.columns)}\")\n",
        "    print(f\"\\nColumns: {', '.join(df_taxi.columns)}\")\n",
        "    \n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(\"2. SCHEMA\")\n",
        "    print(f\"{'='*80}\")\n",
        "    df_taxi.printSchema()\n",
        "    \n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(\"3. SUMMARY STATISTICS (Sample)\")\n",
        "    print(f\"{'='*80}\")\n",
        "    try:\n",
        "        # Use sample to avoid shuffle issues\n",
        "        df_taxi.sample(fraction=0.01).summary().show(truncate=False)\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è  Summary failed: {type(e).__name__}\")\n",
        "        print(\"Try using LAPTOP_DEPLOYMENT=True for better taxi data exploration\")\n",
        "    \n",
        "    df_taxi.unpersist()\n",
        "\n",
        "print(\"\\n‚úì Taxi data exploration complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.6.1 Taxi Visual Distributions\n",
        "\n",
        "Generate quick visualizations for key taxi fields to understand distributions visually."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Taxi visualizations\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Set style\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (15, 7)\n",
        "\n",
        "print(\"Generating taxi distribution plots...\")\n",
        "\n",
        "# Convert to pandas if using Spark\n",
        "if LAPTOP_DEPLOYMENT:\n",
        "    taxi_pd = taxi_df.head(100000) if len(taxi_df) > 100000 else taxi_df  # Sample for speed\n",
        "else:\n",
        "    # Sample for visualization to avoid memory issues\n",
        "    try:\n",
        "        print(\"Sampling taxi data for visualization (1%)...\")\n",
        "        taxi_pd = taxi_df.sample(fraction=0.01, seed=42).toPandas()\n",
        "        print(f\"‚úì Sampled {len(taxi_pd):,} taxi records\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è  Sampling failed: {type(e).__name__}\")\n",
        "        print(\"Visualization may not be available. Try LAPTOP_DEPLOYMENT=True\")\n",
        "        taxi_pd = pd.DataFrame()\n",
        "\n",
        "# Skip visualization if data is empty\n",
        "if len(taxi_pd) == 0:\n",
        "    print(\"‚ö†Ô∏è  Cannot generate visualizations - data sampling failed\")\n",
        "    print(\"\\nüí° SOLUTION: Set LAPTOP_DEPLOYMENT=True at the top of the notebook\")\n",
        "else:\n",
        "    # Create subplots (1x3 grid for 3 taxi plots)\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "    fig.suptitle('Taxi Data Distribution Analysis', fontsize=16, fontweight='bold')\n",
        "\n",
        "    # 1. Trip distance distribution\n",
        "    trip_dist_cols = [col for col in taxi_pd.columns if 'distance' in col.lower()]\n",
        "    if trip_dist_cols:\n",
        "        ax = axes[0]\n",
        "        col_name = trip_dist_cols[0]\n",
        "        dist_clean = taxi_pd[(taxi_pd[col_name] > 0) & (taxi_pd[col_name] < 50)]\n",
        "        ax.hist(dist_clean[col_name], bins=50, edgecolor='black', alpha=0.7)\n",
        "        ax.set_xlabel('Trip Distance (miles)')\n",
        "        ax.set_ylabel('Frequency')\n",
        "        ax.set_title('Trip Distance Distribution')\n",
        "        ax.axvline(dist_clean[col_name].median(), color='red', linestyle='--', label=f'Median: {dist_clean[col_name].median():.2f}')\n",
        "        ax.legend()\n",
        "    else:\n",
        "        axes[0].text(0.5, 0.5, 'Trip distance\\ncolumn not found', ha='center', va='center', fontsize=12)\n",
        "        axes[0].axis('off')\n",
        "\n",
        "    # 2. Passenger count distribution\n",
        "    pass_cols = [col for col in taxi_pd.columns if 'passenger' in col.lower()]\n",
        "    if pass_cols:\n",
        "        ax = axes[1]\n",
        "        col_name = pass_cols[0]\n",
        "        pass_counts = taxi_pd[col_name].value_counts().sort_index().head(10)\n",
        "        ax.bar(pass_counts.index, pass_counts.values)\n",
        "        ax.set_xlabel('Passenger Count')\n",
        "        ax.set_ylabel('Frequency')\n",
        "        ax.set_title('Passenger Count Distribution')\n",
        "    else:\n",
        "        axes[1].text(0.5, 0.5, 'Passenger count\\ncolumn not found', ha='center', va='center', fontsize=12)\n",
        "        axes[1].axis('off')\n",
        "\n",
        "    # 3. Trip amount/fare distribution\n",
        "    amount_cols = [col for col in taxi_pd.columns if 'amount' in col.lower() or 'fare' in col.lower()]\n",
        "    if amount_cols:\n",
        "        ax = axes[2]\n",
        "        col_name = amount_cols[0]\n",
        "        amount_clean = taxi_pd[(taxi_pd[col_name] > 0) & (taxi_pd[col_name] < 100)]\n",
        "        ax.hist(amount_clean[col_name], bins=50, edgecolor='black', alpha=0.7)\n",
        "        ax.set_xlabel('Amount ($)')\n",
        "        ax.set_ylabel('Frequency')\n",
        "        ax.set_title(f'{col_name} Distribution')\n",
        "        ax.axvline(amount_clean[col_name].median(), color='red', linestyle='--', label=f'Median: ${amount_clean[col_name].median():.2f}')\n",
        "        ax.legend()\n",
        "    else:\n",
        "        axes[2].text(0.5, 0.5, 'Amount/fare\\ncolumn not found', ha='center', va='center', fontsize=12)\n",
        "        axes[2].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    print(\"‚úì Taxi distribution plots generated!\")\n",
        "    print(\"\\nüìä Key Observations to Note:\")\n",
        "    print(\"  - Check for outliers in trip distance and fare\")\n",
        "    print(\"  - Verify reasonable passenger counts\")\n",
        "    print(\"  - Assess data quality from distributions\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Data Preparation: Craigslist Cars"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.1 Filter to NY State and 2019-2020"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Filtering Craigslist data to NY state, 2019-2020...\")\n",
        "\n",
        "if LAPTOP_DEPLOYMENT:\n",
        "    # Laptop: Pandas operations\n",
        "    ny_cars_raw = craigslist_df[craigslist_df['state'] == 'ny'].copy()\n",
        "    print(f\"‚úì NY car listings: {len(ny_cars_raw):,}\")\n",
        "    \n",
        "    # Convert date column\n",
        "    ny_cars_raw['posting_date'] = pd.to_datetime(ny_cars_raw['posting_date'], errors='coerce')\n",
        "    \n",
        "    # Show date range\n",
        "    print(f\"\\nDate range: {ny_cars_raw['posting_date'].min()} to {ny_cars_raw['posting_date'].max()}\")\n",
        "    \n",
        "else:\n",
        "    # Cloud: Spark operations\n",
        "    ny_cars_raw = craigslist_df.filter(col(\"state\") == \"ny\")\n",
        "    print(f\"‚úì NY car listings: {ny_cars_raw.count():,}\")\n",
        "    \n",
        "    print(\"\\nDate range in dataset:\")\n",
        "    ny_cars_raw.select(\n",
        "        min(\"posting_date\").alias(\"min_date\"),\n",
        "        max(\"posting_date\").alias(\"max_date\")\n",
        "    ).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Cleaning and filtering to 2019-2020...\")\n",
        "\n",
        "if LAPTOP_DEPLOYMENT:\n",
        "    # Laptop: Pandas operations\n",
        "    ny_cars_cleaned = ny_cars_raw[\n",
        "        ny_cars_raw['price'].notna() &\n",
        "        ny_cars_raw['year'].notna() &\n",
        "        ny_cars_raw['posting_date'].notna()\n",
        "    ].copy()\n",
        "    \n",
        "    # Filter to 2019-2020\n",
        "    ny_cars_cleaned = ny_cars_cleaned[\n",
        "        ny_cars_cleaned['posting_date'].dt.year.isin([2019, 2020])\n",
        "    ]\n",
        "    \n",
        "    # Convert types\n",
        "    ny_cars_cleaned['price'] = pd.to_numeric(ny_cars_cleaned['price'], errors='coerce')\n",
        "    ny_cars_cleaned['year'] = pd.to_numeric(ny_cars_cleaned['year'], errors='coerce').astype('Int64')\n",
        "    ny_cars_cleaned['odometer'] = pd.to_numeric(ny_cars_cleaned['odometer'], errors='coerce')\n",
        "    \n",
        "    print(f\"‚úì After filtering 2019-2020: {len(ny_cars_cleaned):,} records\")\n",
        "    \n",
        "else:\n",
        "    # Cloud: Spark operations\n",
        "    ny_cars_cleaned = ny_cars_raw \\\n",
        "        .filter(col(\"price\").isNotNull()) \\\n",
        "        .filter(col(\"year\").isNotNull()) \\\n",
        "        .filter(col(\"posting_date\").isNotNull()) \\\n",
        "        .withColumn(\"posting_date\", to_timestamp(col(\"posting_date\"))) \\\n",
        "        .filter(year(col(\"posting_date\")).isin([2019, 2020])) \\\n",
        "        .withColumn(\"price\", col(\"price\").cast(\"double\")) \\\n",
        "        .withColumn(\"year\", col(\"year\").cast(\"int\")) \\\n",
        "        .withColumn(\"odometer\", col(\"odometer\").cast(\"double\"))\n",
        "    \n",
        "    print(f\"‚úì After filtering 2019-2020: {ny_cars_cleaned.count():,} records\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.2 Calculate Statistics and Remove Outliers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Calculating price statistics...\")\n",
        "\n",
        "if LAPTOP_DEPLOYMENT:\n",
        "    # Laptop: Pandas statistics\n",
        "    price_stats = {\n",
        "        'mean_price': ny_cars_cleaned['price'].mean(),\n",
        "        'std_price': ny_cars_cleaned['price'].std(),\n",
        "        'min_price': ny_cars_cleaned['price'].min(),\n",
        "        'max_price': ny_cars_cleaned['price'].max(),\n",
        "        'p01': ny_cars_cleaned['price'].quantile(0.01),\n",
        "        'p99': ny_cars_cleaned['price'].quantile(0.99)\n",
        "    }\n",
        "else:\n",
        "    # Cloud: Spark statistics\n",
        "    price_stats = ny_cars_cleaned.select(\n",
        "        mean(\"price\").alias(\"mean_price\"),\n",
        "        stddev(\"price\").alias(\"std_price\"),\n",
        "        expr(\"percentile_approx(price, 0.01)\").alias(\"p01\"),\n",
        "        expr(\"percentile_approx(price, 0.99)\").alias(\"p99\"),\n",
        "        min(\"price\").alias(\"min_price\"),\n",
        "        max(\"price\").alias(\"max_price\")\n",
        "    ).collect()[0].asDict()\n",
        "\n",
        "print(\"\\n=== PRICE STATISTICS ===\")\n",
        "print(f\"Mean: ${price_stats['mean_price']:,.2f}\")\n",
        "print(f\"Std Dev: ${price_stats['std_price']:,.2f}\")\n",
        "print(f\"Min: ${price_stats['min_price']:,.2f}\")\n",
        "print(f\"Max: ${price_stats['max_price']:,.2f}\")\n",
        "print(f\"1st percentile: ${price_stats['p01']:,.2f}\")\n",
        "print(f\"99th percentile: ${price_stats['p99']:,.2f}\")\n",
        "\n",
        "# Define outlier thresholds\n",
        "price_lower = max(500, price_stats['p01'])\n",
        "price_upper = min(75000, price_stats['p99'])\n",
        "\n",
        "print(f\"\\n‚úì Outlier thresholds: ${price_lower:,.2f} - ${price_upper:,.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Removing outliers...\")\n",
        "\n",
        "if LAPTOP_DEPLOYMENT:\n",
        "    # Laptop: Pandas filtering\n",
        "    outliers_mask = (ny_cars_cleaned['price'] < price_lower) | (ny_cars_cleaned['price'] > price_upper)\n",
        "    outliers_removed = ny_cars_cleaned[outliers_mask]\n",
        "    ny_cars = ny_cars_cleaned[~outliers_mask].copy()\n",
        "    \n",
        "    print(f\"\\n=== OUTLIER REMOVAL REPORT ===\")\n",
        "    print(f\"Records before: {len(ny_cars_cleaned):,}\")\n",
        "    print(f\"Outliers removed: {len(outliers_removed):,} ({len(outliers_removed)/len(ny_cars_cleaned)*100:.2f}%)\")\n",
        "    print(f\"Records after: {len(ny_cars):,}\")\n",
        "    \n",
        "    print(\"\\nOutlier breakdown:\")\n",
        "    too_low = (outliers_removed['price'] < price_lower).sum()\n",
        "    too_high = (outliers_removed['price'] > price_upper).sum()\n",
        "    print(f\"  Too low (< ${price_lower:,.0f}): {too_low:,}\")\n",
        "    print(f\"  Too high (> ${price_upper:,.0f}): {too_high:,}\")\n",
        "    \n",
        "else:\n",
        "    # Cloud: Spark filtering\n",
        "    outliers_removed = ny_cars_cleaned.filter(\n",
        "        (col(\"price\") < price_lower) | (col(\"price\") > price_upper)\n",
        "    )\n",
        "    ny_cars = ny_cars_cleaned.filter(\n",
        "        (col(\"price\") >= price_lower) & (col(\"price\") <= price_upper)\n",
        "    )\n",
        "    \n",
        "    print(f\"\\n=== OUTLIER REMOVAL REPORT ===\")\n",
        "    print(f\"Records before: {ny_cars_cleaned.count():,}\")\n",
        "    print(f\"Outliers removed: {outliers_removed.count():,} ({outliers_removed.count()/ny_cars_cleaned.count()*100:.2f}%)\")\n",
        "    print(f\"Records after: {ny_cars.count():,}\")\n",
        "    \n",
        "    print(\"\\nOutlier breakdown:\")\n",
        "    outliers_removed.select(\n",
        "        count(when(col(\"price\") < price_lower, 1)).alias(\"too_low\"),\n",
        "        count(when(col(\"price\") > price_upper, 1)).alias(\"too_high\")\n",
        "    ).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.3 Add Derived Columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Adding derived columns for dimensional analysis...\")\n",
        "\n",
        "if LAPTOP_DEPLOYMENT:\n",
        "    # Laptop: Pandas operations\n",
        "    ny_cars_final = ny_cars.copy()\n",
        "    \n",
        "    # Date dimensions\n",
        "    ny_cars_final['date'] = ny_cars_final['posting_date'].dt.date\n",
        "    ny_cars_final['year_month'] = ny_cars_final['posting_date'].dt.strftime('%Y-%m')\n",
        "    ny_cars_final['year_week'] = ny_cars_final['posting_date'].dt.strftime('%Y-%U')\n",
        "    \n",
        "    # Vehicle age\n",
        "    ny_cars_final['vehicle_age'] = ny_cars_final['posting_date'].dt.year - ny_cars_final['year']\n",
        "    ny_cars_final['age_category'] = pd.cut(\n",
        "        ny_cars_final['vehicle_age'],\n",
        "        bins=[-np.inf, 3, 7, 12, np.inf],\n",
        "        labels=['0-3 years', '4-7 years', '8-12 years', '12+ years']\n",
        "    )\n",
        "    \n",
        "    # Clean text fields\n",
        "    ny_cars_final['vehicle_type_clean'] = ny_cars_final['type'].fillna('unknown').str.lower().str.strip()\n",
        "    ny_cars_final['manufacturer_clean'] = ny_cars_final['manufacturer'].fillna('unknown').str.lower().str.strip()\n",
        "    ny_cars_final['region_clean'] = ny_cars_final['region'].fillna('unknown').str.lower().str.strip()\n",
        "    \n",
        "    print(f\"‚úì Final car dataset: {len(ny_cars_final):,} records\")\n",
        "    print(\"\\nSample:\")\n",
        "    print(ny_cars_final[[\n",
        "        'date', 'year_month', 'price', 'manufacturer_clean',\n",
        "        'vehicle_type_clean', 'age_category', 'region_clean'\n",
        "    ]].head(10))\n",
        "    \n",
        "else:\n",
        "    # Cloud: Spark operations\n",
        "    ny_cars_final = ny_cars \\\n",
        "        .withColumn(\"date\", to_date(col(\"posting_date\"))) \\\n",
        "        .withColumn(\"year_month\", date_format(col(\"posting_date\"), \"yyyy-MM\")) \\\n",
        "        .withColumn(\"year_week\", date_format(col(\"posting_date\"), \"yyyy-ww\")) \\\n",
        "        .withColumn(\"vehicle_age\", year(col(\"posting_date\")) - col(\"year\")) \\\n",
        "        .withColumn(\n",
        "            \"age_category\",\n",
        "            when(col(\"vehicle_age\") <= 3, \"0-3 years\")\n",
        "            .when(col(\"vehicle_age\") <= 7, \"4-7 years\")\n",
        "            .when(col(\"vehicle_age\") <= 12, \"8-12 years\")\n",
        "            .otherwise(\"12+ years\")\n",
        "        ) \\\n",
        "        .withColumn(\n",
        "            \"vehicle_type_clean\",\n",
        "            coalesce(lower(trim(col(\"type\"))), lit(\"unknown\"))\n",
        "        ) \\\n",
        "        .withColumn(\n",
        "            \"manufacturer_clean\",\n",
        "            coalesce(lower(trim(col(\"manufacturer\"))), lit(\"unknown\"))\n",
        "        ) \\\n",
        "        .withColumn(\n",
        "            \"region_clean\",\n",
        "            coalesce(lower(trim(col(\"region\"))), lit(\"unknown\"))\n",
        "        )\n",
        "    \n",
        "    print(\"‚úì Final car dataset prepared\")\n",
        "    ny_cars_final.select(\n",
        "        \"date\", \"year_month\", \"price\", \"manufacturer_clean\",\n",
        "        \"vehicle_type_clean\", \"age_category\", \"region_clean\"\n",
        "    ).show(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Data Preparation: NYC Taxi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.1 Identify Columns and Filter 2019-2020"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Identifying datetime columns...\")\n",
        "\n",
        "if LAPTOP_DEPLOYMENT:\n",
        "    taxi_columns = list(taxi_df.columns)\n",
        "else:\n",
        "    taxi_columns = taxi_df.columns\n",
        "\n",
        "pickup_col = [c for c in taxi_columns if 'pickup' in c.lower() and 'datetime' in c.lower()][0]\n",
        "dropoff_col = [c for c in taxi_columns if 'dropoff' in c.lower() and 'datetime' in c.lower()][0]\n",
        "\n",
        "print(f\"‚úì Using columns: {pickup_col}, {dropoff_col}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Cleaning and filtering taxi data to 2019-2020...\")\n",
        "\n",
        "if LAPTOP_DEPLOYMENT:\n",
        "    # Laptop: Pandas operations\n",
        "    taxi_cleaned = taxi_df[taxi_df[pickup_col].notna()].copy()\n",
        "    \n",
        "    # Convert datetime\n",
        "    taxi_cleaned['pickup_datetime'] = pd.to_datetime(taxi_cleaned[pickup_col], errors='coerce')\n",
        "    taxi_cleaned['dropoff_datetime'] = pd.to_datetime(taxi_cleaned[dropoff_col], errors='coerce')\n",
        "    \n",
        "    # Filter to 2019-2020\n",
        "    taxi_cleaned = taxi_cleaned[\n",
        "        taxi_cleaned['pickup_datetime'].dt.year.isin([2019, 2020]) &\n",
        "        (taxi_cleaned['pickup_datetime'] < taxi_cleaned['dropoff_datetime'])\n",
        "    ]\n",
        "    \n",
        "    print(f\"‚úì Taxi records in 2019-2020: {len(taxi_cleaned):,}\")\n",
        "    \n",
        "else:\n",
        "    # Cloud: Spark operations\n",
        "    taxi_cleaned = taxi_df \\\n",
        "        .filter(col(pickup_col).isNotNull()) \\\n",
        "        .withColumn(\"pickup_datetime\", to_timestamp(col(pickup_col))) \\\n",
        "        .withColumn(\"dropoff_datetime\", to_timestamp(col(dropoff_col))) \\\n",
        "        .filter(year(col(\"pickup_datetime\")).isin([2019, 2020])) \\\n",
        "        .filter(col(\"pickup_datetime\") < col(\"dropoff_datetime\"))\n",
        "    \n",
        "    print(f\"‚úì Taxi records in 2019-2020: {taxi_cleaned.count():,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.2 Remove Distance Outliers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check for distance column\n",
        "distance_col = None\n",
        "for col_name in ['trip_distance', 'distance']:\n",
        "    if col_name in taxi_columns:\n",
        "        distance_col = col_name\n",
        "        break\n",
        "\n",
        "if distance_col:\n",
        "    print(f\"Removing trip distance outliers (using column: {distance_col})...\")\n",
        "    \n",
        "    if LAPTOP_DEPLOYMENT:\n",
        "        # Laptop: Pandas operations\n",
        "        p99_distance = taxi_cleaned[distance_col].quantile(0.99)\n",
        "        max_trip_distance = min(50, p99_distance)\n",
        "        \n",
        "        outliers_count = (\n",
        "            (taxi_cleaned[distance_col] <= 0) | \n",
        "            (taxi_cleaned[distance_col] > max_trip_distance)\n",
        "        ).sum()\n",
        "        \n",
        "        taxi_cleaned = taxi_cleaned[\n",
        "            (taxi_cleaned[distance_col] > 0) & \n",
        "            (taxi_cleaned[distance_col] <= max_trip_distance)\n",
        "        ]\n",
        "        \n",
        "        print(f\"\\n=== TAXI OUTLIER REMOVAL ===\")\n",
        "        print(f\"Outliers removed: {outliers_count:,}\")\n",
        "        print(f\"‚úì Remaining records: {len(taxi_cleaned):,}\")\n",
        "        \n",
        "    else:\n",
        "        # Cloud: Spark operations\n",
        "        taxi_distance_stats = taxi_cleaned.select(\n",
        "            expr(f\"percentile_approx({distance_col}, 0.99)\").alias(\"p99_distance\"),\n",
        "            max(distance_col).alias(\"max_distance\")\n",
        "        ).collect()[0]\n",
        "        \n",
        "        max_trip_distance = min(50, taxi_distance_stats['p99_distance'])\n",
        "        \n",
        "        outliers_taxi = taxi_cleaned.filter(\n",
        "            (col(distance_col) <= 0) | (col(distance_col) > max_trip_distance)\n",
        "        )\n",
        "        \n",
        "        taxi_cleaned = taxi_cleaned.filter(\n",
        "            (col(distance_col) > 0) & (col(distance_col) <= max_trip_distance)\n",
        "        )\n",
        "        \n",
        "        print(f\"\\n=== TAXI OUTLIER REMOVAL ===\")\n",
        "        print(f\"Outliers removed: {outliers_taxi.count():,}\")\n",
        "        print(f\"‚úì Remaining records: {taxi_cleaned.count():,}\")\n",
        "else:\n",
        "    print(\"‚ö† No distance column found, skipping distance outlier removal\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.3 Add Time Dimensions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Adding time dimensions to taxi data...\")\n",
        "\n",
        "if LAPTOP_DEPLOYMENT:\n",
        "    # Laptop: Pandas operations\n",
        "    taxi_final = taxi_cleaned.copy()\n",
        "    \n",
        "    taxi_final['pickup_date'] = taxi_final['pickup_datetime'].dt.date\n",
        "    taxi_final['pickup_year_month'] = taxi_final['pickup_datetime'].dt.strftime('%Y-%m')\n",
        "    taxi_final['pickup_year_week'] = taxi_final['pickup_datetime'].dt.strftime('%Y-%U')\n",
        "    taxi_final['pickup_hour'] = taxi_final['pickup_datetime'].dt.hour\n",
        "    taxi_final['dropoff_date'] = taxi_final['dropoff_datetime'].dt.date\n",
        "    taxi_final['trip_duration_min'] = (\n",
        "        (taxi_final['dropoff_datetime'] - taxi_final['pickup_datetime']).dt.total_seconds() / 60\n",
        "    )\n",
        "    \n",
        "    print(f\"‚úì Taxi dataset prepared: {len(taxi_final):,} records\")\n",
        "    print(\"\\nSample:\")\n",
        "    print(taxi_final[['pickup_date', 'pickup_year_month', 'pickup_hour', 'trip_duration_min']].head(10))\n",
        "    \n",
        "else:\n",
        "    # Cloud: Spark operations\n",
        "    taxi_final = taxi_cleaned \\\n",
        "        .withColumn(\"pickup_date\", to_date(col(\"pickup_datetime\"))) \\\n",
        "        .withColumn(\"pickup_year_month\", date_format(col(\"pickup_datetime\"), \"yyyy-MM\")) \\\n",
        "        .withColumn(\"pickup_year_week\", date_format(col(\"pickup_datetime\"), \"yyyy-ww\")) \\\n",
        "        .withColumn(\"pickup_hour\", hour(col(\"pickup_datetime\"))) \\\n",
        "        .withColumn(\"dropoff_date\", to_date(col(\"dropoff_datetime\"))) \\\n",
        "        .withColumn(\"trip_duration_min\",\n",
        "                    (unix_timestamp(col(\"dropoff_datetime\")) - unix_timestamp(col(\"pickup_datetime\"))) / 60)\n",
        "    \n",
        "    print(\"‚úì Taxi dataset prepared\")\n",
        "    taxi_final.select(\n",
        "        \"pickup_date\", \"pickup_year_month\", \"pickup_hour\", \"trip_duration_min\"\n",
        "    ).show(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Build Dimensional Model - Taxi Demand Facts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Creating taxi demand aggregations...\")\n",
        "\n",
        "if LAPTOP_DEPLOYMENT:\n",
        "    # Laptop: Pandas aggregations\n",
        "    \n",
        "    # Daily pickup density\n",
        "    pickup_density_daily = taxi_final.groupby('pickup_date').agg({\n",
        "        'pickup_datetime': 'count',\n",
        "        'trip_duration_min': 'mean'\n",
        "    }).rename(columns={\n",
        "        'pickup_datetime': 'pickup_trip_count',\n",
        "        'trip_duration_min': 'avg_trip_duration'\n",
        "    }).reset_index().rename(columns={'pickup_date': 'date'})\n",
        "    \n",
        "    # Daily dropoff density\n",
        "    dropoff_density_daily = taxi_final.groupby('dropoff_date').agg({\n",
        "        'dropoff_datetime': 'count'\n",
        "    }).rename(columns={\n",
        "        'dropoff_datetime': 'dropoff_trip_count'\n",
        "    }).reset_index().rename(columns={'dropoff_date': 'date'})\n",
        "    \n",
        "    # Merge\n",
        "    taxi_demand_daily = pickup_density_daily.merge(\n",
        "        dropoff_density_daily, on='date', how='outer'\n",
        "    ).fillna(0)\n",
        "    \n",
        "    taxi_demand_daily['total_trip_count'] = (\n",
        "        taxi_demand_daily['pickup_trip_count'] + taxi_demand_daily['dropoff_trip_count']\n",
        "    )\n",
        "    taxi_demand_daily['date'] = pd.to_datetime(taxi_demand_daily['date'])\n",
        "    taxi_demand_daily['year_month'] = taxi_demand_daily['date'].dt.strftime('%Y-%m')\n",
        "    taxi_demand_daily['year_week'] = taxi_demand_daily['date'].dt.strftime('%Y-%U')\n",
        "    \n",
        "    print(f\"‚úì Daily taxi demand records: {len(taxi_demand_daily):,}\")\n",
        "    print(\"\\nSample:\")\n",
        "    print(taxi_demand_daily.head(10))\n",
        "    \n",
        "else:\n",
        "    # Cloud: Spark aggregations\n",
        "    pickup_density_daily = taxi_final \\\n",
        "        .groupBy(\"pickup_date\") \\\n",
        "        .agg(\n",
        "            count(\"*\").alias(\"pickup_trip_count\"),\n",
        "            avg(\"trip_duration_min\").alias(\"avg_trip_duration\")\n",
        "        ) \\\n",
        "        .withColumn(\"trip_type\", lit(\"pickup\"))\n",
        "    \n",
        "    dropoff_density_daily = taxi_final \\\n",
        "        .groupBy(\"dropoff_date\") \\\n",
        "        .agg(\n",
        "            count(\"*\").alias(\"dropoff_trip_count\"),\n",
        "        ) \\\n",
        "        .withColumnRenamed(\"dropoff_date\", \"date\")\n",
        "    \n",
        "    taxi_demand_daily = pickup_density_daily \\\n",
        "        .withColumnRenamed(\"pickup_date\", \"date\") \\\n",
        "        .join(dropoff_density_daily, \"date\", \"outer\") \\\n",
        "        .fillna(0) \\\n",
        "        .withColumn(\"total_trip_count\", col(\"pickup_trip_count\") + col(\"dropoff_trip_count\")) \\\n",
        "        .withColumn(\"year_month\", date_format(col(\"date\"), \"yyyy-MM\")) \\\n",
        "        .withColumn(\"year_week\", date_format(col(\"date\"), \"yyyy-ww\"))\n",
        "    \n",
        "    print(f\"‚úì Daily taxi demand records: {taxi_demand_daily.count():,}\")\n",
        "    taxi_demand_daily.orderBy(\"date\").show(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Weekly aggregation\n",
        "print(\"Creating weekly aggregation...\")\n",
        "\n",
        "if LAPTOP_DEPLOYMENT:\n",
        "    taxi_demand_weekly = taxi_demand_daily.groupby('year_week').agg({\n",
        "        'pickup_trip_count': 'sum',\n",
        "        'dropoff_trip_count': 'sum',\n",
        "        'total_trip_count': 'sum',\n",
        "        'avg_trip_duration': 'mean'\n",
        "    }).reset_index()\n",
        "    \n",
        "    print(f\"‚úì Weekly taxi demand records: {len(taxi_demand_weekly):,}\")\n",
        "else:\n",
        "    taxi_demand_weekly = taxi_demand_daily \\\n",
        "        .groupBy(\"year_week\") \\\n",
        "        .agg(\n",
        "            sum(\"pickup_trip_count\").alias(\"pickup_trip_count\"),\n",
        "            sum(\"dropoff_trip_count\").alias(\"dropoff_trip_count\"),\n",
        "            sum(\"total_trip_count\").alias(\"total_trip_count\"),\n",
        "            avg(\"avg_trip_duration\").alias(\"avg_trip_duration\")\n",
        "        )\n",
        "    \n",
        "    print(f\"‚úì Weekly taxi demand records: {taxi_demand_weekly.count():,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Monthly aggregation\n",
        "print(\"Creating monthly aggregation...\")\n",
        "\n",
        "if LAPTOP_DEPLOYMENT:\n",
        "    taxi_demand_monthly = taxi_demand_daily.groupby('year_month').agg({\n",
        "        'pickup_trip_count': 'sum',\n",
        "        'dropoff_trip_count': 'sum',\n",
        "        'total_trip_count': 'sum',\n",
        "        'avg_trip_duration': 'mean',\n",
        "        'date': 'count'\n",
        "    }).rename(columns={'date': 'days_in_period'}).reset_index()\n",
        "    \n",
        "    taxi_demand_monthly['avg_daily_trips'] = (\n",
        "        taxi_demand_monthly['total_trip_count'] / taxi_demand_monthly['days_in_period']\n",
        "    )\n",
        "    \n",
        "    print(f\"‚úì Monthly taxi demand records: {len(taxi_demand_monthly):,}\")\n",
        "    print(\"\\nSample:\")\n",
        "    print(taxi_demand_monthly)\n",
        "else:\n",
        "    taxi_demand_monthly = taxi_demand_daily \\\n",
        "        .groupBy(\"year_month\") \\\n",
        "        .agg(\n",
        "            sum(\"pickup_trip_count\").alias(\"pickup_trip_count\"),\n",
        "            sum(\"dropoff_trip_count\").alias(\"dropoff_trip_count\"),\n",
        "            sum(\"total_trip_count\").alias(\"total_trip_count\"),\n",
        "            avg(\"avg_trip_duration\").alias(\"avg_trip_duration\"),\n",
        "            count(\"date\").alias(\"days_in_period\")\n",
        "        ) \\\n",
        "        .withColumn(\"avg_daily_trips\", col(\"total_trip_count\") / col(\"days_in_period\"))\n",
        "    \n",
        "    print(f\"‚úì Monthly taxi demand records: {taxi_demand_monthly.count():,}\")\n",
        "    taxi_demand_monthly.orderBy(\"year_month\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Build Dimensional Model - Car Price Facts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Creating car price aggregations...\")\n",
        "\n",
        "if LAPTOP_DEPLOYMENT:\n",
        "    # Laptop: Pandas aggregations\n",
        "    \n",
        "    # Daily\n",
        "    car_price_daily = ny_cars_final.groupby('date').agg({\n",
        "        'price': ['median', 'mean', 'count', lambda x: x.quantile(0.25), lambda x: x.quantile(0.75)]\n",
        "    })\n",
        "    car_price_daily.columns = ['median_price', 'avg_price', 'listing_count', 'p25_price', 'p75_price']\n",
        "    car_price_daily = car_price_daily.reset_index()\n",
        "    car_price_daily['date'] = pd.to_datetime(car_price_daily['date'])\n",
        "    car_price_daily['year_month'] = car_price_daily['date'].dt.strftime('%Y-%m')\n",
        "    car_price_daily['year_week'] = car_price_daily['date'].dt.strftime('%Y-%U')\n",
        "    \n",
        "    print(f\"‚úì Daily car price records: {len(car_price_daily):,}\")\n",
        "    \n",
        "    # Weekly\n",
        "    car_price_weekly = ny_cars_final.groupby('year_week').agg({\n",
        "        'price': ['median', 'mean', 'count']\n",
        "    })\n",
        "    car_price_weekly.columns = ['median_price', 'avg_price', 'listing_count']\n",
        "    car_price_weekly = car_price_weekly.reset_index()\n",
        "    \n",
        "    print(f\"‚úì Weekly car price records: {len(car_price_weekly):,}\")\n",
        "    \n",
        "    # Monthly\n",
        "    car_price_monthly = ny_cars_final.groupby('year_month').agg({\n",
        "        'price': ['median', 'mean', 'count', lambda x: x.quantile(0.25), lambda x: x.quantile(0.75)]\n",
        "    })\n",
        "    car_price_monthly.columns = ['median_price', 'avg_price', 'listing_count', 'p25_price', 'p75_price']\n",
        "    car_price_monthly = car_price_monthly.reset_index()\n",
        "    \n",
        "    print(f\"‚úì Monthly car price records: {len(car_price_monthly):,}\")\n",
        "    print(\"\\nSample:\")\n",
        "    print(car_price_monthly)\n",
        "    \n",
        "else:\n",
        "    # Cloud: Spark aggregations\n",
        "    \n",
        "    # Daily\n",
        "    car_price_daily = ny_cars_final \\\n",
        "        .groupBy(\"date\") \\\n",
        "        .agg(\n",
        "            expr(\"percentile_approx(price, 0.5)\").alias(\"median_price\"),\n",
        "            avg(\"price\").alias(\"avg_price\"),\n",
        "            count(\"*\").alias(\"listing_count\"),\n",
        "            expr(\"percentile_approx(price, 0.25)\").alias(\"p25_price\"),\n",
        "            expr(\"percentile_approx(price, 0.75)\").alias(\"p75_price\")\n",
        "        ) \\\n",
        "        .withColumn(\"year_month\", date_format(col(\"date\"), \"yyyy-MM\")) \\\n",
        "        .withColumn(\"year_week\", date_format(col(\"date\"), \"yyyy-ww\"))\n",
        "    \n",
        "    print(f\"‚úì Daily car price records: {car_price_daily.count():,}\")\n",
        "    \n",
        "    # Weekly\n",
        "    car_price_weekly = ny_cars_final \\\n",
        "        .groupBy(\"year_week\") \\\n",
        "        .agg(\n",
        "            expr(\"percentile_approx(price, 0.5)\").alias(\"median_price\"),\n",
        "            avg(\"price\").alias(\"avg_price\"),\n",
        "            count(\"*\").alias(\"listing_count\")\n",
        "        )\n",
        "    \n",
        "    print(f\"‚úì Weekly car price records: {car_price_weekly.count():,}\")\n",
        "    \n",
        "    # Monthly\n",
        "    car_price_monthly = ny_cars_final \\\n",
        "        .groupBy(\"year_month\") \\\n",
        "        .agg(\n",
        "            expr(\"percentile_approx(price, 0.5)\").alias(\"median_price\"),\n",
        "            avg(\"price\").alias(\"avg_price\"),\n",
        "            count(\"*\").alias(\"listing_count\"),\n",
        "            expr(\"percentile_approx(price, 0.25)\").alias(\"p25_price\"),\n",
        "            expr(\"percentile_approx(price, 0.75)\").alias(\"p75_price\")\n",
        "        )\n",
        "    \n",
        "    print(f\"‚úì Monthly car price records: {car_price_monthly.count():,}\")\n",
        "    car_price_monthly.orderBy(\"year_month\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Car price by vehicle type (monthly)\n",
        "print(\"Creating price by vehicle type...\")\n",
        "\n",
        "if LAPTOP_DEPLOYMENT:\n",
        "    car_price_by_type_monthly = ny_cars_final.groupby(['year_month', 'vehicle_type_clean']).agg({\n",
        "        'price': ['median', 'count']\n",
        "    })\n",
        "    car_price_by_type_monthly.columns = ['median_price', 'listing_count']\n",
        "    car_price_by_type_monthly = car_price_by_type_monthly.reset_index()\n",
        "    car_price_by_type_monthly = car_price_by_type_monthly[\n",
        "        car_price_by_type_monthly['listing_count'] >= 10\n",
        "    ]\n",
        "    \n",
        "    print(f\"‚úì Car price by type (monthly): {len(car_price_by_type_monthly):,}\")\n",
        "else:\n",
        "    car_price_by_type_monthly = ny_cars_final \\\n",
        "        .groupBy(\"year_month\", \"vehicle_type_clean\") \\\n",
        "        .agg(\n",
        "            expr(\"percentile_approx(price, 0.5)\").alias(\"median_price\"),\n",
        "            count(\"*\").alias(\"listing_count\")\n",
        "        ) \\\n",
        "        .filter(col(\"listing_count\") >= 10)\n",
        "    \n",
        "    print(f\"‚úì Car price by type (monthly): {car_price_by_type_monthly.count():,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Car price by age category (monthly)\n",
        "print(\"Creating price by age category...\")\n",
        "\n",
        "if LAPTOP_DEPLOYMENT:\n",
        "    car_price_by_age_monthly = ny_cars_final.groupby(['year_month', 'age_category']).agg({\n",
        "        'price': ['median', 'count']\n",
        "    })\n",
        "    car_price_by_age_monthly.columns = ['median_price', 'listing_count']\n",
        "    car_price_by_age_monthly = car_price_by_age_monthly.reset_index()\n",
        "    car_price_by_age_monthly = car_price_by_age_monthly[\n",
        "        car_price_by_age_monthly['listing_count'] >= 10\n",
        "    ]\n",
        "    \n",
        "    print(f\"‚úì Car price by age (monthly): {len(car_price_by_age_monthly):,}\")\n",
        "else:\n",
        "    car_price_by_age_monthly = ny_cars_final \\\n",
        "        .groupBy(\"year_month\", \"age_category\") \\\n",
        "        .agg(\n",
        "            expr(\"percentile_approx(price, 0.5)\").alias(\"median_price\"),\n",
        "            count(\"*\").alias(\"listing_count\")\n",
        "        ) \\\n",
        "        .filter(col(\"listing_count\") >= 10)\n",
        "    \n",
        "    print(f\"‚úì Car price by age (monthly): {car_price_by_age_monthly.count():,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Combine Fact Tables (Join)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Joining taxi demand and car price facts...\")\n",
        "\n",
        "if LAPTOP_DEPLOYMENT:\n",
        "    # Laptop: Pandas merge\n",
        "    \n",
        "    # Daily\n",
        "    combined_daily = taxi_demand_daily.merge(\n",
        "        car_price_daily[['date', 'median_price', 'avg_price', 'listing_count', 'p25_price', 'p75_price']],\n",
        "        on='date',\n",
        "        how='inner'\n",
        "    ).sort_values('date')\n",
        "    \n",
        "    print(f\"‚úì Combined daily records: {len(combined_daily):,}\")\n",
        "    \n",
        "    # Weekly\n",
        "    combined_weekly = taxi_demand_weekly.merge(\n",
        "        car_price_weekly,\n",
        "        on='year_week',\n",
        "        how='inner'\n",
        "    ).sort_values('year_week')\n",
        "    \n",
        "    print(f\"‚úì Combined weekly records: {len(combined_weekly):,}\")\n",
        "    \n",
        "    # Monthly\n",
        "    combined_monthly = taxi_demand_monthly.merge(\n",
        "        car_price_monthly,\n",
        "        on='year_month',\n",
        "        how='inner'\n",
        "    )\n",
        "    combined_monthly = combined_monthly.rename(columns={\n",
        "        'pickup_trip_count': 'monthly_pickups',\n",
        "        'dropoff_trip_count': 'monthly_dropoffs',\n",
        "        'total_trip_count': 'monthly_total_trips'\n",
        "    }).sort_values('year_month')\n",
        "    \n",
        "    print(f\"‚úì Combined monthly records: {len(combined_monthly):,}\")\n",
        "    print(\"\\nMonthly combined data:\")\n",
        "    print(combined_monthly)\n",
        "    \n",
        "else:\n",
        "    # Cloud: Spark join\n",
        "    \n",
        "    # Daily\n",
        "    combined_daily = taxi_demand_daily \\\n",
        "        .join(car_price_daily, \"date\", \"inner\") \\\n",
        "        .select(\n",
        "            \"date\",\n",
        "            \"year_month\",\n",
        "            \"year_week\",\n",
        "            \"pickup_trip_count\",\n",
        "            \"dropoff_trip_count\",\n",
        "            \"total_trip_count\",\n",
        "            \"avg_trip_duration\",\n",
        "            \"median_price\",\n",
        "            \"avg_price\",\n",
        "            \"listing_count\",\n",
        "            \"p25_price\",\n",
        "            \"p75_price\"\n",
        "        ) \\\n",
        "        .orderBy(\"date\")\n",
        "    \n",
        "    print(f\"‚úì Combined daily records: {combined_daily.count():,}\")\n",
        "    \n",
        "    # Weekly\n",
        "    combined_weekly = taxi_demand_weekly \\\n",
        "        .join(car_price_weekly, \"year_week\", \"inner\") \\\n",
        "        .orderBy(\"year_week\")\n",
        "    \n",
        "    print(f\"‚úì Combined weekly records: {combined_weekly.count():,}\")\n",
        "    \n",
        "    # Monthly\n",
        "    combined_monthly = taxi_demand_monthly \\\n",
        "        .join(car_price_monthly, \"year_month\", \"inner\") \\\n",
        "        .select(\n",
        "            \"year_month\",\n",
        "            col(\"pickup_trip_count\").alias(\"monthly_pickups\"),\n",
        "            col(\"dropoff_trip_count\").alias(\"monthly_dropoffs\"),\n",
        "            col(\"total_trip_count\").alias(\"monthly_total_trips\"),\n",
        "            \"avg_daily_trips\",\n",
        "            \"avg_trip_duration\",\n",
        "            \"median_price\",\n",
        "            \"avg_price\",\n",
        "            \"listing_count\",\n",
        "            \"p25_price\",\n",
        "            \"p75_price\"\n",
        "        ) \\\n",
        "        .orderBy(\"year_month\")\n",
        "    \n",
        "    print(f\"‚úì Combined monthly records: {combined_monthly.count():,}\")\n",
        "    combined_monthly.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Convert to Pandas for Analysis & Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Converting to Pandas for correlation analysis and visualization...\")\n",
        "\n",
        "if LAPTOP_DEPLOYMENT:\n",
        "    # Already in Pandas, just assign\n",
        "    combined_daily_pd = combined_daily\n",
        "    combined_weekly_pd = combined_weekly\n",
        "    combined_monthly_pd = combined_monthly\n",
        "    price_by_type_pd = car_price_by_type_monthly\n",
        "    price_by_age_pd = car_price_by_age_monthly\n",
        "else:\n",
        "    # Convert from Spark to Pandas\n",
        "    combined_daily_pd = combined_daily.toPandas()\n",
        "    combined_weekly_pd = combined_weekly.toPandas()\n",
        "    combined_monthly_pd = combined_monthly.toPandas()\n",
        "    price_by_type_pd = car_price_by_type_monthly.toPandas()\n",
        "    price_by_age_pd = car_price_by_age_monthly.toPandas()\n",
        "\n",
        "print(\"‚úì Data ready for analysis\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Correlation Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\" * 60)\n",
        "print(\"CORRELATION ANALYSIS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(\"\\n1. Daily Granularity:\")\n",
        "print(f\"   Pickup trips vs Median price: {combined_daily_pd['pickup_trip_count'].corr(combined_daily_pd['median_price']):.4f}\")\n",
        "print(f\"   Dropoff trips vs Median price: {combined_daily_pd['dropoff_trip_count'].corr(combined_daily_pd['median_price']):.4f}\")\n",
        "print(f\"   Total trips vs Median price: {combined_daily_pd['total_trip_count'].corr(combined_daily_pd['median_price']):.4f}\")\n",
        "\n",
        "print(\"\\n2. Weekly Granularity:\")\n",
        "print(f\"   Pickup trips vs Median price: {combined_weekly_pd['pickup_trip_count'].corr(combined_weekly_pd['median_price']):.4f}\")\n",
        "print(f\"   Dropoff trips vs Median price: {combined_weekly_pd['dropoff_trip_count'].corr(combined_weekly_pd['median_price']):.4f}\")\n",
        "print(f\"   Total trips vs Median price: {combined_weekly_pd['total_trip_count'].corr(combined_weekly_pd['median_price']):.4f}\")\n",
        "\n",
        "print(\"\\n3. Monthly Granularity:\")\n",
        "print(f\"   Monthly pickups vs Median price: {combined_monthly_pd['monthly_pickups'].corr(combined_monthly_pd['median_price']):.4f}\")\n",
        "print(f\"   Monthly dropoffs vs Median price: {combined_monthly_pd['monthly_dropoffs'].corr(combined_monthly_pd['median_price']):.4f}\")\n",
        "print(f\"   Monthly total trips vs Median price: {combined_monthly_pd['monthly_total_trips'].corr(combined_monthly_pd['median_price']):.4f}\")\n",
        "print(f\"   Avg daily trips vs Median price: {combined_monthly_pd['avg_daily_trips'].corr(combined_monthly_pd['median_price']):.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Lagged correlation analysis\n",
        "combined_monthly_pd_sorted = combined_monthly_pd.sort_values('year_month').copy()\n",
        "\n",
        "print(\"\\n=== LAGGED CORRELATION ANALYSIS ===\")\n",
        "print(\"(Does taxi demand predict future car prices?)\\n\")\n",
        "\n",
        "for lag in [1, 2, 3]:\n",
        "    combined_monthly_pd_sorted[f'median_price_lag_{lag}'] = combined_monthly_pd_sorted['median_price'].shift(lag)\n",
        "    corr = combined_monthly_pd_sorted['monthly_total_trips'].corr(\n",
        "        combined_monthly_pd_sorted[f'median_price_lag_{lag}']\n",
        "    )\n",
        "    print(f\"Lag {lag} month: Trips vs Price: {corr:.4f}\")\n",
        "\n",
        "print(\"\\n(Does car price predict future taxi demand?)\\n\")\n",
        "for lag in [1, 2, 3]:\n",
        "    combined_monthly_pd_sorted[f'trips_lag_{lag}'] = combined_monthly_pd_sorted['monthly_total_trips'].shift(lag)\n",
        "    corr = combined_monthly_pd_sorted['median_price'].corr(\n",
        "        combined_monthly_pd_sorted[f'trips_lag_{lag}']\n",
        "    )\n",
        "    print(f\"Lag {lag} month: Price vs Trips: {corr:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Summary Statistics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"DATASET SUMMARY\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(f\"\\nTime period: 2019-2020\")\n",
        "\n",
        "if LAPTOP_DEPLOYMENT:\n",
        "    print(f\"Taxi trips analyzed: {len(taxi_final):,}\")\n",
        "    print(f\"NY car listings analyzed: {len(ny_cars_final):,}\")\n",
        "else:\n",
        "    print(f\"Taxi trips analyzed: {taxi_final.count():,}\")\n",
        "    print(f\"NY car listings analyzed: {ny_cars_final.count():,}\")\n",
        "\n",
        "print(f\"\\nDaily observations: {len(combined_daily_pd):,}\")\n",
        "print(f\"Weekly observations: {len(combined_weekly_pd):,}\")\n",
        "print(f\"Monthly observations: {len(combined_monthly_pd):,}\")\n",
        "\n",
        "print(\"\\n=== KEY INSIGHTS ===\")\n",
        "print(f\"Average daily taxi trips: {combined_monthly_pd['avg_daily_trips'].mean():,.0f}\")\n",
        "print(f\"Average median car price: ${combined_monthly_pd['median_price'].mean():,.2f}\")\n",
        "print(f\"Price range: ${combined_monthly_pd['median_price'].min():,.2f} - ${combined_monthly_pd['median_price'].max():,.2f}\")\n",
        "print(f\"Trip volume range: {combined_monthly_pd['monthly_total_trips'].min():,.0f} - {combined_monthly_pd['monthly_total_trips'].max():,.0f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 13. Export Combined Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Saving combined datasets...\")\n",
        "\n",
        "if LAPTOP_DEPLOYMENT:\n",
        "    # Laptop: Save as parquet using Pandas\n",
        "    combined_daily_pd.to_parquet('data/combined_daily.parquet', index=False)\n",
        "    combined_weekly_pd.to_parquet('data/combined_weekly.parquet', index=False)\n",
        "    combined_monthly_pd.to_parquet('data/combined_monthly.parquet', index=False)\n",
        "    price_by_type_pd.to_parquet('data/price_by_type_monthly.parquet', index=False)\n",
        "    price_by_age_pd.to_parquet('data/price_by_age_monthly.parquet', index=False)\n",
        "else:\n",
        "    # Cloud: Save using Spark\n",
        "    combined_daily.write.mode('overwrite').parquet('data/combined_daily.parquet')\n",
        "    combined_weekly.write.mode('overwrite').parquet('data/combined_weekly.parquet')\n",
        "    combined_monthly.write.mode('overwrite').parquet('data/combined_monthly.parquet')\n",
        "    car_price_by_type_monthly.write.mode('overwrite').parquet('data/price_by_type_monthly.parquet')\n",
        "    car_price_by_age_monthly.write.mode('overwrite').parquet('data/price_by_age_monthly.parquet')\n",
        "\n",
        "print(\"‚úì Combined datasets saved to parquet files\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 14. Visualizations\n",
        "\n",
        "The visualization code below works the same regardless of deployment mode (since we've converted to Pandas)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Graph 1: Time Series Overlay"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare data\n",
        "combined_monthly_pd['year_month_dt'] = pd.to_datetime(combined_monthly_pd['year_month'])\n",
        "\n",
        "# Create figure with secondary y-axis\n",
        "fig = go.Figure()\n",
        "\n",
        "# Add taxi trips trace\n",
        "fig.add_trace(go.Scatter(\n",
        "    x=combined_monthly_pd['year_month_dt'],\n",
        "    y=combined_monthly_pd['monthly_total_trips'],\n",
        "    name='Total Taxi Trips',\n",
        "    line=dict(color='blue', width=2),\n",
        "    yaxis='y'\n",
        "))\n",
        "\n",
        "# Add car price trace\n",
        "fig.add_trace(go.Scatter(\n",
        "    x=combined_monthly_pd['year_month_dt'],\n",
        "    y=combined_monthly_pd['median_price'],\n",
        "    name='Median Car Price',\n",
        "    line=dict(color='red', width=2),\n",
        "    yaxis='y2'\n",
        "))\n",
        "\n",
        "# Update layout\n",
        "fig.update_layout(\n",
        "    title='NYC Taxi Demand vs Used Car Prices Over Time (2019-2020)',\n",
        "    xaxis=dict(title='Month'),\n",
        "    yaxis=dict(\n",
        "        title='Total Taxi Trips',\n",
        "        titlefont=dict(color='blue'),\n",
        "        tickfont=dict(color='blue')\n",
        "    ),\n",
        "    yaxis2=dict(\n",
        "        title='Median Car Price ($)',\n",
        "        titlefont=dict(color='red'),\n",
        "        tickfont=dict(color='red'),\n",
        "        overlaying='y',\n",
        "        side='right'\n",
        "    ),\n",
        "    hovermode='x unified',\n",
        "    height=500\n",
        ")\n",
        "\n",
        "fig.show()\n",
        "\n",
        "print(\"\\nGraph 1: If hypothesis is true, you should see the two lines moving in similar patterns.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Graph 2: Scatter Plot with Trend Line"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create scatter plot\n",
        "fig = px.scatter(\n",
        "    combined_monthly_pd,\n",
        "    x='monthly_total_trips',\n",
        "    y='median_price',\n",
        "    color='year_month_dt',\n",
        "    trendline='ols',\n",
        "    labels={\n",
        "        'monthly_total_trips': 'Total Monthly Taxi Trips',\n",
        "        'median_price': 'Median Car Price ($)',\n",
        "        'year_month_dt': 'Month'\n",
        "    },\n",
        "    title='Correlation: Taxi Trip Volume vs Car Prices',\n",
        "    hover_data=['year_month']\n",
        ")\n",
        "\n",
        "fig.update_layout(height=500)\n",
        "fig.show()\n",
        "\n",
        "corr_val = combined_monthly_pd['monthly_total_trips'].corr(combined_monthly_pd['median_price'])\n",
        "print(f\"\\nGraph 2: Correlation coefficient = {corr_val:.4f}\")\n",
        "print(\"If hypothesis is true, expect positive correlation (upward trend line).\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Graph 3: Correlation Heatmap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Select numeric columns for correlation\n",
        "corr_cols = [\n",
        "    'monthly_pickups', 'monthly_dropoffs', 'monthly_total_trips',\n",
        "    'avg_daily_trips', 'avg_trip_duration', 'median_price', 'avg_price'\n",
        "]\n",
        "\n",
        "corr_matrix = combined_monthly_pd[corr_cols].corr()\n",
        "\n",
        "# Create heatmap\n",
        "fig = go.Figure(data=go.Heatmap(\n",
        "    z=corr_matrix.values,\n",
        "    x=corr_matrix.columns,\n",
        "    y=corr_matrix.columns,\n",
        "    colorscale='RdBu',\n",
        "    zmid=0,\n",
        "    text=corr_matrix.values,\n",
        "    texttemplate='%{text:.3f}',\n",
        "    textfont={\"size\": 10}\n",
        "))\n",
        "\n",
        "fig.update_layout(\n",
        "    title='Correlation Matrix: All Variables',\n",
        "    height=600,\n",
        "    xaxis={'side': 'bottom'}\n",
        ")\n",
        "\n",
        "fig.show()\n",
        "\n",
        "print(\"\\nGraph 3: Look for strong correlations (close to +1 or -1) between trip metrics and price.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Graph 4: Lagged Correlation Plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate lagged correlations\n",
        "lags = list(range(0, 6))\n",
        "trips_to_price_corr = []\n",
        "price_to_trips_corr = []\n",
        "\n",
        "combined_monthly_sorted = combined_monthly_pd.sort_values('year_month')\n",
        "\n",
        "for lag in lags:\n",
        "    if lag == 0:\n",
        "        trips_to_price_corr.append(\n",
        "            combined_monthly_sorted['monthly_total_trips'].corr(combined_monthly_sorted['median_price'])\n",
        "        )\n",
        "        price_to_trips_corr.append(\n",
        "            combined_monthly_sorted['median_price'].corr(combined_monthly_sorted['monthly_total_trips'])\n",
        "        )\n",
        "    else:\n",
        "        trips_to_price_corr.append(\n",
        "            combined_monthly_sorted['monthly_total_trips'].iloc[:-lag].corr(\n",
        "                combined_monthly_sorted['median_price'].iloc[lag:]\n",
        "            )\n",
        "        )\n",
        "        price_to_trips_corr.append(\n",
        "            combined_monthly_sorted['median_price'].iloc[:-lag].corr(\n",
        "                combined_monthly_sorted['monthly_total_trips'].iloc[lag:]\n",
        "            )\n",
        "        )\n",
        "\n",
        "# Plot\n",
        "fig = go.Figure()\n",
        "\n",
        "fig.add_trace(go.Scatter(\n",
        "    x=lags,\n",
        "    y=trips_to_price_corr,\n",
        "    mode='lines+markers',\n",
        "    name='Trips ‚Üí Price (trips lead)',\n",
        "    line=dict(color='blue', width=2)\n",
        "))\n",
        "\n",
        "fig.add_trace(go.Scatter(\n",
        "    x=lags,\n",
        "    y=price_to_trips_corr,\n",
        "    mode='lines+markers',\n",
        "    name='Price ‚Üí Trips (price leads)',\n",
        "    line=dict(color='red', width=2)\n",
        "))\n",
        "\n",
        "fig.add_hline(y=0, line_dash=\"dash\", line_color=\"gray\")\n",
        "\n",
        "fig.update_layout(\n",
        "    title='Lagged Correlation: Does One Variable Predict the Other?',\n",
        "    xaxis_title='Lag (months)',\n",
        "    yaxis_title='Correlation Coefficient',\n",
        "    height=500\n",
        ")\n",
        "\n",
        "fig.show()\n",
        "\n",
        "print(\"\\nGraph 4: If one line peaks at lag > 0, that variable predicts the other.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Graph 5: Vehicle Type & Age Breakdown"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Vehicle type trends\n",
        "top_types = price_by_type_pd.groupby('vehicle_type_clean')['listing_count'].sum().nlargest(6).index\n",
        "price_by_type_filtered = price_by_type_pd[price_by_type_pd['vehicle_type_clean'].isin(top_types)]\n",
        "\n",
        "fig = px.line(\n",
        "    price_by_type_filtered,\n",
        "    x='year_month',\n",
        "    y='median_price',\n",
        "    color='vehicle_type_clean',\n",
        "    title='Median Car Price Over Time by Vehicle Type',\n",
        "    labels={'median_price': 'Median Price ($)', 'year_month': 'Month'},\n",
        "    height=500\n",
        ")\n",
        "\n",
        "fig.update_xaxes(tickangle=45)\n",
        "fig.show()\n",
        "\n",
        "# Vehicle age trends\n",
        "fig = px.line(\n",
        "    price_by_age_pd,\n",
        "    x='year_month',\n",
        "    y='median_price',\n",
        "    color='age_category',\n",
        "    title='Median Car Price Over Time by Vehicle Age',\n",
        "    labels={'median_price': 'Median Price ($)', 'year_month': 'Month'},\n",
        "    height=500\n",
        ")\n",
        "\n",
        "fig.update_xaxes(tickangle=45)\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 15. Analysis Complete"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(f\"  ANALYSIS COMPLETE ({' LAPTOP MODE' if LAPTOP_DEPLOYMENT else 'CLOUD MODE'})\")\n",
        "print(\"=\"*80)\n",
        "print(\"\\nNext steps:\")\n",
        "print(\"1. Review all graphs above\")\n",
        "print(\"2. Look for consistent patterns across multiple visualizations\")\n",
        "print(\"3. Consider alternative explanations for any correlations found\")\n",
        "print(\"4. Focus on pre-COVID data (2019) for cleaner signal\")\n",
        "print(\"5. Be aware of geographic mismatch (NYC taxi vs NY state cars)\")\n",
        "print(\"\\n‚ö†Ô∏è  Remember: Correlation ‚â† Causation\")\n",
        "print(\"\\n‚úì Output files saved to data/ directory\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
